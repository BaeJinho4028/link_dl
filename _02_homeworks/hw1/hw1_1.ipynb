{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40ca1754-b164-422d-9e5f-640038927238",
   "metadata": {},
   "source": [
    "# a_tensor_initialization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04ba502e-e05b-4e13-a914-d438a2bafd4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T13:05:57.101727Z",
     "start_time": "2024-09-24T13:05:53.641237Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca23556-f625-4991-83a3-def5c86bb833",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T13:06:06.010799Z",
     "start_time": "2024-09-24T13:06:05.966660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor class\n",
    "t1 = torch.Tensor([1, 2, 3], device='cpu')\n",
    "print(t1.dtype)   # >>> torch.float32\n",
    "print(t1.device)  # >>> cpu\n",
    "print(t1.requires_grad)  # >>> False\n",
    "print(t1.size())  # torch.Size([3])\n",
    "print(t1.shape)   # torch.Size([3])\n",
    "\n",
    "# if you have gpu device\n",
    "# t1_cuda = t1.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t1_cuda = t1.cuda()\n",
    "t1_cpu = t1.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364e4b87-5eda-4b50-b050-d5226c2345c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T13:06:06.132782Z",
     "start_time": "2024-09-24T13:06:06.120424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.tensor([1, 2, 3], device='cpu')\n",
    "print(t2.dtype)  # 데이터 타입: torch.int64\n",
    "print(t2.device)  # 디바이스: cpu\n",
    "print(t2.requires_grad)  # 미분 활성화 여부: False\n",
    "print(t2.size())  # 크기: torch.Size([3])\n",
    "print(t2.shape)  # 모양: torch.Size([3])\n",
    "\n",
    "# GPU가 있을 경우, 텐서를 CUDA 디바이스로 전송하는 코드\n",
    "# t2_cuda = t2.to(torch.device('cuda'))\n",
    "# t2_cuda = t2.cuda()\n",
    "\n",
    "# CPU로 변환\n",
    "t2_cpu = t2.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3c5f083-d108-4504-9671-d01b1d804624",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T13:06:07.759193Z",
     "start_time": "2024-09-24T13:06:06.515223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n",
      "torch.Size([3, 2]) 2\n",
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n",
      "torch.Size([3, 1, 2, 3]) 4\n",
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 3 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(a10\u001b[38;5;241m.\u001b[39mshape, a10\u001b[38;5;241m.\u001b[39mndim)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# 마지막 텐서는 각 3차원 텐서의 길이가 맞지 않아 ValueError를 발생시킵니다.\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m a11 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# ValueError: expected sequence of length 3 at dim 3 (got 2)\u001b[39;49;00m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 3 at dim 3 (got 2)"
     ]
    }
   ],
   "source": [
    "# 1차원 텐서 (벡터)\n",
    "a2 = torch.tensor([1])  # 크기: torch.Size([1]), 차원: 1\n",
    "print(a2.shape, a2.ndim)\n",
    "\n",
    "# 1차원 텐서 (5개의 원소를 가진 벡터)\n",
    "a3 = torch.tensor([1, 2, 3, 4, 5])  # 크기: torch.Size([5]), 차원: 1\n",
    "print(a3.shape, a3.ndim)\n",
    "\n",
    "# 2차원 텐서 (5x1)\n",
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])  # 크기: torch.Size([5, 1]), 차원: 2\n",
    "print(a4.shape, a4.ndim)\n",
    "\n",
    "# 2차원 텐서 (3x2)\n",
    "a5 = torch.tensor([  # 크기: torch.Size([3, 2]), 차원: 2\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(a5.shape, a5.ndim)\n",
    "\n",
    "# 3차원 텐서 (3x2x1)\n",
    "a6 = torch.tensor([  # 크기: torch.Size([3, 2, 1]), 차원: 3\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)\n",
    "\n",
    "#4차원 텐서 (3x1x2x1)\n",
    "a7 = torch.tensor([  # 크기: torch.Size([3, 1, 2, 1]), 차원: 4\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)\n",
    "\n",
    "# 4차원 텐서 (3x1x2x3)\n",
    "a8 = torch.tensor([  # 크기: torch.Size([3, 1, 2, 3]), 차원: 4\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)\n",
    "\n",
    "# 5차원 텐서 (3x1x2x3x1)\n",
    "a9 = torch.tensor([  # 크기: torch.Size([3, 1, 2, 3, 1]), 차원: 5\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)\n",
    "\n",
    "# 2차원 텐서 (4x5)\n",
    "a10 = torch.tensor([  # 크기: torch.Size([4, 5]), 차원: 2\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "# 3차원 텐서 (4x1x5)\n",
    "a10 = torch.tensor([  # 크기: torch.Size([4, 1, 5]), 차원: 3\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "# 마지막 텐서는 각 3차원 텐서의 길이가 맞지 않아 ValueError를 발생시킵니다.\n",
    "a11 = torch.tensor([                 # ValueError: expected sequence of length 3 at dim 3 (got 2)\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beffd419-050b-461d-955a-ea9ebcac7cf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T13:06:07.759193Z",
     "start_time": "2024-09-24T13:06:06.515223Z"
    }
   },
   "source": [
    "텐서를 생성하는 다양한 방법을 소개하며, 각 방법의 데이터 타입, 장치, 크기 등을 확인할 수 있다.\n",
    "torch.Tensor와 torch.tensor의 차이점에 유의해야 하며, torch.as_tensor는 리스트나 배열을 참조하여 생성한다.\n",
    "GPU 장치에서의 텐서 생성 및 변환 과정도 함께 다루고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d24eb5-f825-4c1c-a272-93e15afdaa9c",
   "metadata": {},
   "source": [
    "# b_tensor_initialization_copy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "150dd918-aec6-4ddf-9dbe-1cb6f25a526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "704572aa-1e92-499e-9b64-edb51f200920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 리스트를 이용한 텐서 생성\n",
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1)\n",
    "\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2)\n",
    "\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3) # 참조\n",
    "\n",
    "# 리스트의 첫 번째 값 변경\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "# 텐서 출력\n",
    "print(t1)  # Tensor([1., 2., 3.])\n",
    "print(t2)  # Tensor([1, 2, 3])\n",
    "print(t3)  # Tensor([100, 2, 3]) -> 리스트 l3의 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10075a18-c8b5-47f1-b3ad-9b8bd12f96f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([100,   2,   3])\n"
     ]
    }
   ],
   "source": [
    "# NumPy 배열을 이용한 텐서 생성\n",
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch.Tensor(l4)\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6) # 참조\n",
    "\n",
    "# NumPy 배열의 첫 번째 값 변경\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "# 텐서 출력b\n",
    "print(t4)  # Tensor([1., 2., 3.])\n",
    "print(t5)  # Tensor([1, 2, 3])\n",
    "print(t6)  # Tensor([100, 2, 3]) -> NumPy 배열 l6의 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2de747-7fbb-48e2-87c6-651b1d0f0d8a",
   "metadata": {},
   "source": [
    "리스트와 NumPy 배열을 사용하여 텐서를 생성할 때의 차이를 설명한다.\n",
    "특히 torch.as_tensor를 사용하면 원본 데이터에 대한 참조가 생성되어, 원본 데이터 변경 시 텐서도 영향을 받게 된다.\n",
    "이러한 특성을 이해하는 것이 메모리 관리 및 데이터 일관성 유지에 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7269c-55a1-4605-bcc1-2073f9c20fec",
   "metadata": {},
   "source": [
    "# c_tensor_initialization_constant_values.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2af9880d-e981-4b31-bd3f-8fe254e364e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 5개의 1로 채워진 텐서 생성\n",
    "t1 = torch.ones(size=(5,))  # or torch.ones(5)\n",
    "t1_like = torch.ones_like(input=t1)\n",
    "print(t1)       \n",
    "print(t1_like)  \n",
    "\n",
    "# 6개의 0으로 채워진 텐서 생성\n",
    "t2 = torch.zeros(size=(6,))  # or torch.zeros(6)\n",
    "t2_like = torch.zeros_like(input=t2)\n",
    "print(t2)       \n",
    "print(t2_like)  \n",
    "\n",
    "# 4개의 초기화되지 않은 텐서 생성\n",
    "t3 = torch.empty(size=(4,))  # or torch.empty(4)\n",
    "t3_like = torch.empty_like(input=t3)\n",
    "print(t3)       # 초기화되지 않은 불규칙한 값\n",
    "print(t3_like)  # 초기화되지 않은 불규칙한 값\n",
    "\n",
    "# 3x3 단위 행렬 생성\n",
    "t4 = torch.eye(n=3)\n",
    "print(t4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba683a5-184e-470a-afa5-918a35bfe42f",
   "metadata": {},
   "source": [
    "상수로 채워진 텐서를 생성하는 방법을 소개하며, torch.ones, torch.zeros, torch.empty의 용도와 초기화 상태를 비교한다.\n",
    "단위 행렬을 생성하는 torch.eye 함수도 사용되며, 다양한 초기화 방법의 선택이 학습 모델 성능에 미치는 영향을 이해하는 데 도움이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d058fa72-2bf1-46d9-888e-81442d5eda83",
   "metadata": {},
   "source": [
    "# d_tensor_initialization_random_values.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45fda214-20af-4e5d-b177-34f52d8541b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 11]])\n",
      "tensor([[0.4900, 0.9904, 0.7808]])\n",
      "tensor([[-1.2219,  0.0555,  0.1962]])\n",
      "tensor([[ 9.2344,  9.8247],\n",
      "        [ 9.1812,  9.7243],\n",
      "        [10.2509,  9.4246]])\n",
      "tensor([0.0000, 2.5000, 5.0000])\n",
      "tensor([0, 1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# 10부터 20 사이의 정수를 가지는 랜덤 텐서 생성\n",
    "t1 = torch.randint(low=10, high=20, size=(1, 2))\n",
    "print(t1)  # >>> tensor([[x, y]])  # x, y는 10~19 사이의 정수\n",
    "\n",
    "# 0과 1 사이의 실수를 가지는 랜덤 텐서 생성\n",
    "t2 = torch.rand(size=(1, 3))\n",
    "print(t2) \n",
    "\n",
    "# 평균 0, 표준편차 1인 정규 분포에서 랜덤한 실수를 가지는 텐서 생성\n",
    "t3 = torch.randn(size=(1, 3))\n",
    "print(t3)  \n",
    "\n",
    "# 평균 10, 표준편차 1인 정규 분포에서 랜덤한 실수를 가지는 텐서 생성\n",
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2))\n",
    "print(t4) \n",
    "\n",
    "# 0부터 5까지 균등한 간격으로 3개의 값을 생성\n",
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3)\n",
    "print(t5)  # >>> tensor([0.0000, 2.5000, 5.0000])\n",
    "\n",
    "# 0부터 4까지 1씩 증가하는 정수로 텐서를 생성\n",
    "t6 = torch.arange(5)\n",
    "print(t6)  # >>> tensor([0, 1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b7bf46a-eec7-48f6-b949-06a4c71a392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "# 시드를 설정한 후 랜덤 텐서 생성\n",
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1) \n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2) \n",
    "\n",
    "print()\n",
    "\n",
    "# 같은 시드를 설정하면 동일한 난수 시퀀스를 재현 가능\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)  # random1과 동일한 결과\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)  # random2와 동일한 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e11cfbe-6b60-4391-9029-047ccf2c29f4",
   "metadata": {},
   "source": [
    "랜덤 텐서를 생성하는 다양한 방법을 다룬다. 각 텐서 생성 함수의 용도와 그에 따른 값의 분포를 설명한다.\n",
    "랜덤 시드를 설정하는 과정은 실험의 재현성을 보장하기 위해 필수적이다.\n",
    "특히, torch.randint, torch.rand, torch.randn 등을 활용하여 난수 생성의 다양성을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15449266-9d35-44ca-8673-e56c14d88689",
   "metadata": {},
   "source": [
    "# e_tensor_type_conversion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be7390bb-6d59-4607-954c-03a5b65c9ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[18.0429,  7.2532, 19.6519],\n",
      "        [10.8626,  2.1505, 19.6913]], dtype=torch.float64)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# 기본 float32 타입으로 ones 텐서 생성\n",
    "a = torch.ones((2, 3))\n",
    "print(a.dtype)  # >>> torch.float32\n",
    "\n",
    "# int16 타입으로 ones 텐서 생성\n",
    "b = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(b)  # >>> tensor([[1, 1, 1], [1, 1, 1]], dtype=torch.int16)\n",
    "\n",
    "# float64 타입으로 0과 1 사이의 랜덤 값을 생성하고, 20을 곱해 범위를 확장\n",
    "c = torch.rand((2, 3), dtype=torch.float64) * 20.0\n",
    "print(c)  # >>> 예시: tensor([[12.5, 19.4, 2.3], [6.8, 17.9, 4.5]], dtype=torch.float64)\n",
    "\n",
    "# b 텐서를 int32 타입으로 변환\n",
    "d = b.to(torch.int32)\n",
    "print(d)  # >>> tensor([[1, 1, 1], [1, 1, 1]], dtype=torch.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5db7e5ce-beac-4027-824e-fed907905cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.int16\n"
     ]
    }
   ],
   "source": [
    "# double 타입의 텐서 생성\n",
    "double_d = torch.ones(10, 2, dtype=torch.double)\n",
    "# short 타입의 텐서 생성\n",
    "short_e = torch.tensor([[1, 2]], dtype=torch.short)\n",
    "\n",
    "# zeros로 double 타입 텐서 생성\n",
    "double_d = torch.zeros(10, 2).double()\n",
    "# ones로 short 타입 텐서 생성\n",
    "short_e = torch.ones(10, 2).short()\n",
    "\n",
    "# to()를 사용한 데이터 타입 변환\n",
    "double_d = torch.zeros(10, 2).to(torch.double)\n",
    "short_e = torch.ones(10, 2).to(dtype=torch.short)\n",
    "\n",
    "# type()을 사용한 데이터 타입 변환\n",
    "double_d = torch.zeros(10, 2).type(torch.double)\n",
    "short_e = torch.ones(10, 2).type(dtype=torch.short)\n",
    "\n",
    "# dtype 출력\n",
    "print(double_d.dtype)  # >>> torch.float64\n",
    "print(short_e.dtype)   # >>> torch.int16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77060a93-2c97-44da-bf93-6c0139ffafbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "# double 타입의 텐서 생성\n",
    "double_f = torch.rand(5, dtype=torch.double)\n",
    "# short 타입으로 변환\n",
    "short_g = double_f.to(torch.short)\n",
    "# double_f와 short_g의 곱의 결과의 데이터 타입 확인\n",
    "print((double_f * short_g).dtype)  # >>> torch.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d6d602-9bd0-4b7d-b56a-f939a678be91",
   "metadata": {},
   "source": [
    "텐서의 데이터 타입을 변환하는 방법을 설명한다. 데이터 타입에 따른 메모리 사용량 및 연산 성능 차이를 인식하는 것이 중요하다.\n",
    "type()과 to() 메소드를 통해 다양한 데이터 타입을 설정하는 방식이 구체적으로 다뤄진다.\n",
    "변환 후의 텐서에서 연산 결과의 데이터 타입도 확인하여 타입 변환의 효과를 이해할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772d012-7c46-44f7-96ca-4ca1827e6662",
   "metadata": {},
   "source": [
    "# f_tensor_operations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba21fdb7-3fd7-4e4a-9e1a-cc18f27733b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 더하기\n",
    "t1 = torch.ones(size=(2, 3))\n",
    "t2 = torch.ones(size=(2, 3))\n",
    "t3 = torch.add(t1, t2)\n",
    "t4 = t1 + t2\n",
    "print(t3)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a74b9f3-fb82-47e7-81b1-f99b74446a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 빼기\n",
    "t5 = torch.sub(t1, t2)\n",
    "t6 = t1 - t2\n",
    "print(t5)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d955858-000f-44a9-99c6-ebe3c73100ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 곱하기\n",
    "t7 = torch.mul(t1, t2)\n",
    "t8 = t1 * t2\n",
    "print(t7)\n",
    "print(t8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2be11a8-4e81-4071-abba-ff7073dddffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 나누기\n",
    "t9 = torch.div(t1, t2)\n",
    "t10 = t1 / t2\n",
    "print(t9)\n",
    "print(t10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8844a6-3dff-4022-a2d5-ca1a6dad050e",
   "metadata": {},
   "source": [
    "텐서 간의 기본적인 산술 연산을 소개하며, 연산의 결과를 통해 텐서의 조작 방식과 그 의미를 이해할 수 있다.\n",
    "더하기, 빼기, 곱하기, 나누기 연산을 각각 명시적으로 호출하는 방법과 연산자를 통한 간결한 표현을 모두 보여준다.\n",
    "텐서 연산의 메커니즘을 이해하는 것은 신경망의 기본 구조와 동작을 이해하는 데 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324fe55-3786-49e6-ba90-ddbce8a41cad",
   "metadata": {},
   "source": [
    "# g_tensor_operations_mm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaadab98-9689-44eb-994b-f29af6942ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n",
      "tensor([[1.6750, 2.2840],\n",
      "        [0.0956, 1.0294]]) torch.Size([2, 2])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# 벡터의 내적\n",
    "t1 = torch.dot(\n",
    "    torch.tensor([2, 3]), torch.tensor([2, 1])\n",
    ")\n",
    "print(t1, t1.size())  # 내적 결과와 크기\n",
    "\n",
    "# 행렬의 곱\n",
    "t2 = torch.randn(2, 3)\n",
    "t3 = torch.randn(3, 2)\n",
    "t4 = torch.mm(t2, t3)\n",
    "print(t4, t4.size())  # 행렬 곱 결과와 크기\n",
    "\n",
    "# 배치 행렬 곱\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(10, 4, 5)\n",
    "t7 = torch.bmm(t5, t6)\n",
    "print(t7.size())  # 배치 행렬 곱 결과의 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711dc92a-4ebb-4063-b70e-26e1042c1d3c",
   "metadata": {},
   "source": [
    "내적과 행렬 곱의 개념을 다루며, 벡터와 행렬 간의 연산을 통한 차원 변환을 명확하게 보여준다.\n",
    "torch.dot과 torch.mm을 활용하여 내적 및 행렬 곱을 수행하는 방법을 제시하며, 배치 행렬 곱을 통한 다차원 텐서 연산의 개념도 소개된다.\n",
    "이 과정에서 텐서의 차원에 따른 연산 결과의 크기를 파악하는 것이 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf6b26b-010d-44a4-a75b-42d52e7eece5",
   "metadata": {},
   "source": [
    "# h_tensor_operations_matmul.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d129a8e7-b829-4225-a4da-1a73304e02dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# vector x vector: dot product  # 벡터 x 벡터: 내적\n",
    "t1 = torch.randn(3)\n",
    "t2 = torch.randn(3)\n",
    "print(torch.matmul(t1, t2).size())  # torch.Size([])  # 벡터 간의 내적 결과는 스칼라\n",
    "\n",
    "# matrix x vector: broadcasted dot  # 행렬 x 벡터: 브로드캐스트 내적\n",
    "t3 = torch.randn(3, 4)\n",
    "t4 = torch.randn(4)\n",
    "print(torch.matmul(t3, t4).size())  # torch.Size([3])  # 행렬과 벡터의 곱 결과는 벡터\n",
    "\n",
    "# batched matrix x vector: broadcasted dot  # 배치된 행렬 x 벡터: 브로드캐스트 내적\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(4)\n",
    "print(torch.matmul(t5, t6).size())  # torch.Size([10, 3])  # 배치된 행렬과 벡터의 곱 결과는 배치된 벡터\n",
    "\n",
    "# batched matrix x batched matrix: bmm  # 배치된 행렬 x 배치된 행렬: 배치 행렬 곱\n",
    "t7 = torch.randn(10, 3, 4)\n",
    "t8 = torch.randn(10, 4, 5)\n",
    "print(torch.matmul(t7, t8).size())  # torch.Size([10, 3, 5])  # 배치된 행렬 간의 곱 결과는 배치된 행렬\n",
    "\n",
    "# batched matrix x matrix: bmm  # 배치된 행렬 x 일반 행렬: 배치 행렬 곱\n",
    "t9 = torch.randn(10, 3, 4)\n",
    "t10 = torch.randn(4, 5)\n",
    "print(torch.matmul(t9, t10).size())  # torch.Size([10, 3, 5])  # 배치된 행렬과 일반 행렬의 곱 결과는 배치된 행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576d3eb6-020f-448a-9ffd-490f47b4f1e9",
   "metadata": {},
   "source": [
    "torch.matmul을 사용하여 다양한 형태의 텐서 곱을 수행한다. \n",
    "벡터와 행렬, 배치된 텐서 간의 곱의 결과 차원을 보여주며, 브로드캐스팅 개념을 활용한 연산을 강조한다.\n",
    "다양한 곱셈 방식을 이해하는 것은 복잡한 신경망 아키텍처에서의 텐서 흐름을 관리하는 데 필수적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8227b42b-defd-40e0-aa69-2a6cb870e552",
   "metadata": {},
   "source": [
    "# i_tensor_broadcasting.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db380786-fb0b-4ec4-b81f-fe13403d4728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = 2.0\n",
    "print(t1 * t2)  # t1에 2.0을 곱함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c54fe9d6-c1fe-4220-b4ca-91a0d92a2630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n"
     ]
    }
   ],
   "source": [
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "t4 = torch.tensor([4, 5])\n",
    "print(t3 - t4)  # t3에서 t4를 빼기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e78f243-20a2-412f-9d07-105a15470e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(t5 + 2.0)  # t5에 2.0을 더함\n",
    "print(t5 - 2.0)  # t5에서 2.0을 뺌\n",
    "print(t5 * 2.0)  # t5에 2.0을 곱함\n",
    "print(t5 / 2.0)  # t5를 2.0으로 나눔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "592d2a68-467e-4526-8352-3faf2d23fcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "  return x / 255  # 입력 텐서를 255로 나누어 정규화\n",
    "\n",
    "t6 = torch.randn(3, 28, 28)\n",
    "print(normalize(t6).size())  # 정규화된 텐서의 크기 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "97989c79-1351-4af2-bd9b-f298be2c4821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n"
     ]
    }
   ],
   "source": [
    "t7 = torch.tensor([[1, 2], [0, 3]])  # torch.Size([2, 2])\n",
    "t8 = torch.tensor([[3, 1]])  # torch.Size([1, 2])\n",
    "t9 = torch.tensor([[5], [2]])  # torch.Size([2, 1])\n",
    "t10 = torch.tensor([7])  # torch.Size([1])\n",
    "print(t7 + t8)\n",
    "print(t7 + t9) \n",
    "print(t8 + t9)   \n",
    "print(t7 + t10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff9e159d-4ba0-435c-bac9-27c7dde10ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)  # 3차원과 2차원 텐서 간의 브로드캐스팅\n",
    "print(t12.shape)  # 결과 텐서의 크기 출력\n",
    "\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)  # 3차원 텐서에 1차원 텐서 브로드캐스팅\n",
    "print(t14.shape)  # 결과 텐서의 크기 출력\n",
    "\n",
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)  # 1차원 텐서 브로드캐스팅\n",
    "print(t16.shape)  # 결과 텐서의 크기 출력\n",
    "\n",
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(3, 1, 1)  # 3차원과 1차원 텐서 간의 브로드캐스팅\n",
    "print((t17 + t18).size())  # 결과 텐서의 크기 출력\n",
    "\n",
    "# 곱해도 더해도 크기는 변하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3014ab18-a425-4b39-82ed-bf42baa88e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())  # torch.Size([5, 3, 4, 1])  # 브로드캐스팅된 결과 크기 출력\n",
    "\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())  # torch.Size([3, 1, 7])  # 브로드캐스팅된 결과 크기 출력\n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())  # torch.Size([3, 3, 3])  # 브로드캐스팅된 결과 크기 출력\n",
    "\n",
    "# t25 = torch.empty(5, 2, 4, 1)\n",
    "# t26 = torch.empty(3, 1, 1)\n",
    "# print((t25 + t26).size())\n",
    "# RuntimeError: The size of tensor a (2) must match\n",
    "# the size of tensor b (3) at non-singleton dimension 1\n",
    "# 브로드캐스팅을 위해서는 두 차원의 크기가 같거나 하나의 크기가 1이어야 하는데 아니라서 런타임 오류 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "885f28a2-0356-41d4-a973-4b609bc55e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "source": [
    "t27 = torch.ones(4) * 5\n",
    "print(t27)  # >>> tensor([ 5, 5, 5, 5])  # 모든 요소가 5인 텐서\n",
    "\n",
    "t28 = torch.pow(t27, 2)\n",
    "print(t28)  # >>> tensor([ 25, 25, 25, 25])  # 각 요소를 제곱한 결과\n",
    "\n",
    "exp = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "a = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "t29 = torch.pow(a, exp)\n",
    "print(t29)  # >>> tensor([   1.,    4.,   27.,  256.])  # 각 요소를 지수로 제곱한 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9370942-199f-4221-b1ae-e5102fe5c66c",
   "metadata": {},
   "source": [
    "텐서의 크기와 형태를 다루며, shape과 size 속성의 차이점에 대해 설명한다.\n",
    "이러한 속성은 텐서 연산 및 모델 설계에서 중요한 역할을 하며, 특정 차원에 따라 연산이 어떻게 진행되는지를 이해하는 데 필요하다.\n",
    "다양한 텐서의 차원을 조작하는 연습이 포함되어 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa4f7a-cac6-4a2f-bc5f-d92d4776525a",
   "metadata": {},
   "source": [
    "# j_tensor_indexing_slicing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b68fbb09-31f6-4ab6-a4bd-22cb5df9fd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n"
     ]
    }
   ],
   "source": [
    "# 2D 텐서 생성\n",
    "x = torch.tensor(\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "print(x[1])  # >>> tensor([5, 6, 7, 8, 9]) # 첫 번째 행 선택\n",
    "print(x[:, 1])  # >>> tensor([1, 6, 11]) # 두 번째 열 선택\n",
    "print(x[1, 2])  # >>> tensor(7) # 특정 요소 선택 (1행 2열)\n",
    "print(x[:, -1])  # >>> tensor([4, 9, 14]) # 마지막 열 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c0ea76a-16f6-4e5b-afd5-78513e9ab20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n"
     ]
    }
   ],
   "source": [
    "print(x[1:])  # >>> tensor([[ 5,  6,  7,  8,  9], [10, 11, 12, 13, 14]]) # 1행부터 끝까지 선택\n",
    "print(x[1:, 3:])  # >>> tensor([[ 8,  9], [13, 14]]) # 1행부터 끝까지, 3열부터 끝까지 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c57effa9-6ca6-4809-83be-49f3405d2d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.zeros((6, 6)) # 6x6 크기의 제로 텐서 생성\n",
    "y[1:4, 2] = 1 # 1행부터 3행까지 2열에 1 할당\n",
    "print(y)\n",
    "\n",
    "print(y[1:4, 1:4]) # >>> tensor([[1., 1., 0.], [0., 0., 0.], [0., 0., 0.]]) # 1행부터 3행까지, 1열부터 3열까지 부분 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20638cce-2f94-4d9c-8662-d1bb89de93d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.tensor(\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "\n",
    "print(z[:2])  # >>> tensor([[1, 2, 3, 4], [2, 3, 4, 5]]) # 0행부터 1행까지 선택\n",
    "print(z[1:, 1:3])  # >>> tensor([[3, 4], [6, 7]]) # 1행부터 끝까지, 1열부터 3열까지 선택\n",
    "print(z[:, 1:])  # >>> tensor([[2, 3, 4], [3, 4, 5], [6, 7, 8]]) # 모든 행의 1열부터 끝까지 선택\n",
    "\n",
    "z[1:, 1:3] = 0 # 1행부터 끝까지의 1열과 2열에 0 할당\n",
    "print(z)  # 변경된 텐서 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce50513-46c7-426b-9dba-8e23b0980229",
   "metadata": {},
   "source": [
    "텐서의 인덱싱과 슬라이싱을 통해 특정 원소에 접근하는 방법을 설명한다.\n",
    "다양한 슬라이싱 기법을 통해 텐서에서 원하는 부분을 쉽게 추출할 수 있으며, 이 과정에서 복사본 생성 여부에 대한 주의가 필요하다.\n",
    "인덱싱 기법은 모델의 특정 부분을 조작할 때 매우 유용하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebda461-61bc-4ffd-8939-bdbf985fb3ae",
   "metadata": {},
   "source": [
    "# k_tensor_reshaping.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ea7885c-e4cd-41f0-911e-e13934fea41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = t1.view(3, 2)  # Shape becomes (3, 2), view는 메모리 재배치를 하지 않고 새로운 모양으로 반환\n",
    "t3 = t1.reshape(1, 6)  # Shape becomes (1, 6), reshape은 유연하게 모양을 바꿔서 반환\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.arange(8).view(2, 4)  # Shape becomes (2, 4), 연속된 값을 2x4로 보기\n",
    "t5 = torch.arange(6).view(2, 3)  # Shape becomes (2, 3), 6개의 값을 2x3으로 변형\n",
    "print(t4)\n",
    "print(t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c0ef5b8-c8a0-4a06-9a09-ad2018d335f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (1, 3, 1)\n",
    "t6 = torch.tensor([[[1], [2], [3]]])\n",
    "\n",
    "# Remove all dimensions of size 1\n",
    "t7 = t6.squeeze()  # Shape becomes (3,), 크기가 1인 차원 모두 제거\n",
    "# Remove dimension at position 0\n",
    "t8 = t6.squeeze(0)  # Shape becomes (3, 1), 첫 번째 차원 제거\n",
    "print(t7)\n",
    "print(t8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1a7967c-1421-447b-989b-8853288af552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (3,)\n",
    "t9 = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Add a new dimension at position 1\n",
    "t10 = t9.unsqueeze(1)  # Shape becomes (3, 1), 두 번째 차원에 새로운 차원 추가\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t12 = t11.unsqueeze(1)  # Shape becomes (2, 1, 3), 첫 번째 차원에 새로운 차원 추가\n",
    "print(t12, t12.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dcdb28dc-3150-43c6-8dcc-03f9e0216862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (2, 3)\n",
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Flatten the tensor\n",
    "t14 = t13.flatten()  # Shape becomes (6,), 모든 요소를 1차원으로 평탄화\n",
    "print(t14)\n",
    "\n",
    "# Original tensor with shape (2, 2, 2)\n",
    "t15 = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "t16 = torch.flatten(t15)  # 모든 차원 평탄화\n",
    "t17 = torch.flatten(t15, start_dim=1)  # 두 번째 차원부터 평탄화\n",
    "print(t16)\n",
    "print(t17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6d157aa-5fd7-462f-accd-8ed6ad242fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "t18 = torch.randn(2, 3, 5)  # 무작위 텐서 생성, 모양은 (2, 3, 5)\n",
    "print(t18.shape)  # >>> torch.Size([2, 3, 5])\n",
    "print(torch.permute(t18, (2, 0, 1)).size())  # >>> torch.Size([5, 2, 3]), 차원 순서 변경\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Permute the dimensions\n",
    "t20 = torch.permute(t19, dims=(0, 1))  # Shape becomes (2, 3), 동일한 차원 유지\n",
    "t21 = torch.permute(t19, dims=(1, 0))  # Shape becomes (3, 2), 차원 순서 변경\n",
    "print(t20)\n",
    "print(t21)\n",
    "\n",
    "# Transpose the tensor\n",
    "t22 = torch.transpose(t19, 0, 1)  # Shape becomes (3, 2), 두 차원을 교환\n",
    "print(t22)\n",
    "\n",
    "t23 = torch.t(t19)  # Shape becomes (3, 2), 2차원 텐서 전치 (transpose와 동일)\n",
    "print(t23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b465214-d150-4b4b-aa5d-20cd461a315b",
   "metadata": {},
   "source": [
    "여러 텐서를 연결(concatenate)하는 방법을 설명하며, 각 차원에서의 결합 방식을 다룬다.\n",
    "연결 후 텐서의 차원 변화와 결과를 확인할 수 있으며, 모델의 입력 데이터 구성을 이해하는 데 도움이 된다.\n",
    "concat 연산의 사용은 모델 구조 설계에서 중요한 요소로 작용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c576f68f-b78a-4af6-920d-5e183e2767b5",
   "metadata": {},
   "source": [
    "# l_tensor_concat.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fbdfff8-44c9-4947-b95b-388734cfc905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.zeros([2, 1, 3])  \n",
    "t2 = torch.zeros([2, 3, 3])  \n",
    "t3 = torch.zeros([2, 2, 3])  \n",
    "\n",
    "# 두 번째 차원(dim=1)이 다른 크기를 가질 수 있으며, 이 차원을 따라 합쳐짐.\n",
    "t4 = torch.cat([t1, t2, t3], dim=1)  # 텐서의 모양을 (2, 6, 3)으로 변경\n",
    "print(t4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10559c59-13f1-497c-b92b-011cd47803dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t6 = torch.arange(3, 8)  # tensor([3, 4, 5, 6, 7])\n",
    "\n",
    "t7 = torch.cat((t5, t6), dim=0)  # 텐서의 모양을 (8,)로 변경\n",
    "print(t7.shape)  \n",
    "print(t7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf997337-2a88-4b36-ac48-d5b514ba444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "t8 = torch.arange(0, 6).reshape(2, 3)  # 텐서의 모양을 (2, 3)으로 변경\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)  # 텐서의 모양을 (2, 3)으로 변경\n",
    "\n",
    "# 2차원 텐서간 병합\n",
    "t10 = torch.cat((t8, t9), dim=0)  # 텐서의 모양을 (4, 3)으로 변경\n",
    "print(t10.size())  \n",
    "print(t10)\n",
    "\n",
    "t11 = torch.cat((t8, t9), dim=1)  # 텐서의 모양을 (2, 6)으로 변경\n",
    "print(t11.size())  \n",
    "print(t11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1371b5b7-d56d-4c40-8c13-de320d5e9704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n"
     ]
    }
   ],
   "source": [
    "t12 = torch.arange(0, 6).reshape(2, 3)  # 텐서의 모양을 (2, 3)으로 변경\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)  # 텐서의 모양을 (2, 3)으로 변경\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)  # 텐서의 모양을 (2, 3)으로 변경\n",
    "\n",
    "t15 = torch.cat((t12, t13, t14), dim=0)  # 텐서의 모양을 (6, 3)으로 변경\n",
    "print(t15.size())  \n",
    "print(t15)\n",
    "\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)  # 텐서의 모양을 (2, 9)로 변경\n",
    "print(t16.size())  \n",
    "print(t16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd74a7a0-6a30-4c40-b67d-d09daeb0e537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)  # 텐서의 모양을 (1, 2, 3)으로 변경\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)  # 텐서의 모양을 (1, 2, 3)으로 변경\n",
    "\n",
    "t19 = torch.cat((t17, t18), dim=0)  # 텐서의 모양을 (2, 2, 3)으로 변경\n",
    "print(t19.size())  \n",
    "print(t19)\n",
    "\n",
    "t20 = torch.cat((t17, t18), dim=1)  # 텐서의 모양을 (1, 4, 3)으로 변경\n",
    "print(t20.size())  \n",
    "print(t20)\n",
    "\n",
    "t21 = torch.cat((t17, t18), dim=2)  # 텐서의 모양을 (1, 2, 6)으로 변경\n",
    "print(t21.size())  \n",
    "print(t21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2417442f-bc82-44a9-b914-802c9bb48f57",
   "metadata": {},
   "source": [
    "텐서의 형태를 변형(reshape)하는 방법을 다룬다.\n",
    "reshape와 view 메소드를 통해 텐서의 차원을 조정하고, 이 과정에서 데이터의 연속성을 이해하는 것이 중요하다.\n",
    "텐서의 형태를 조정하는 기술은 데이터 전처리 및 신경망 아키텍처 설계에서 필수적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60cc092-fc9e-43df-b9aa-a5d29b404479",
   "metadata": {},
   "source": [
    "# m_tensor_stacking.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a566aa0e-2ac6-48dc-85ec-de38e7f3e566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# stack은 새로운 차원을 추가하여 텐서를 병합 (dim=0에서 2차원으로)\n",
    "t3 = torch.stack([t1, t2], dim=0)  # 모양: (2, 2, 3)\n",
    "# cat은 기존 차원에서 연결 (unsqueeze로 차원 추가 후 병합)\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)  # 모양: (2, 2, 3)\n",
    "print(t3.shape, t3.equal(t4))\n",
    "\n",
    "# dim=1에서 병합\n",
    "t5 = torch.stack([t1, t2], dim=1)  # 모양: (2, 2, 3)\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)  # 모양: (2, 2, 3)\n",
    "print(t5.shape, t5.equal(t6))\n",
    "\n",
    "# dim=2에서 병합\n",
    "t7 = torch.stack([t1, t2], dim=2)  # 모양: (2, 3, 2)\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)  # 모양: (2, 3, 2)\n",
    "print(t7.shape, t7.equal(t8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea65ab24-897a-49c6-accf-ec268c634739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 1D 텐서 병합 (dim=0에서 병합)\n",
    "t9 = torch.arange(0, 3)  # 모양: (3,)\n",
    "t10 = torch.arange(3, 6)  # 모양: (3,)\n",
    "t11 = torch.stack((t9, t10), dim=0)  # 모양: (2, 3)\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)  # 모양: (2, 3)\n",
    "print(t11.equal(t12))  # True\n",
    "\n",
    "# dim=1에서 병합\n",
    "t13 = torch.stack((t9, t10), dim=1)  # 모양: (3, 2)\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)  # 모양: (3, 2)\n",
    "print(t13.equal(t14))  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61b824d-524c-410f-bf60-722891dad255",
   "metadata": {},
   "source": [
    "텐서의 전치(transpose)를 통해 차원의 순서를 변경하는 방법을 다룬다.\n",
    "전치 연산은 행렬 연산에서 매우 중요하며, 특정 수학적 연산의 결과를 변경할 수 있다.\n",
    "텐서의 전치 기술을 이해하는 것은 선형대수와 머신러닝 모델 설계에 필수적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f35335-28f8-4a94-9ee1-dd9ce7ea78a0",
   "metadata": {},
   "source": [
    "# n_tensor_vstack_hstack.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7243a21-eda0-4ce5-82f0-fdd4084e953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1, 2, 3])  # 1차원 텐서, shape은 (3,)\n",
    "t2 = torch.tensor([4, 5, 6])  # 1차원 텐서, shape은 (3,)\n",
    "\n",
    "# t1과 t2를 수직으로 쌓음 (dim=0을 따라), 결과적으로 (2, 3) 모양의 텐서 생성\n",
    "t3 = torch.vstack((t1, t2))\n",
    "print(t3)\n",
    "# >>> tensor([[1, 2, 3],\n",
    "#             [4, 5, 6]])\n",
    "\n",
    "t4 = torch.tensor([[1], [2], [3]])  # 2차원 텐서, shape은 (3, 1)\n",
    "t5 = torch.tensor([[4], [5], [6]])  # 2차원 텐서, shape은 (3, 1)\n",
    "\n",
    "# t4와 t5를 수직으로 쌓음, 결과적으로 (6, 1) 모양의 텐서 생성\n",
    "t6 = torch.vstack((t4, t5))\n",
    "print(t6)\n",
    "# >>> tensor([[1],\n",
    "#             [2],\n",
    "#             [3],\n",
    "#             [4],\n",
    "#             [5],\n",
    "#             [6]])\n",
    "\n",
    "# 3차원 텐서, shape은 (2, 2, 3)\n",
    "t7 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t7.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "# 3차원 텐서, shape은 (2, 2, 3)\n",
    "t8 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t8.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "# t7과 t8을 수직으로 쌓아 (4, 2, 3) 모양의 텐서 생성\n",
    "t9 = torch.vstack([t7, t8])\n",
    "print(t9.shape)\n",
    "# >>> (4, 2, 3)\n",
    "\n",
    "print(t9)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12]],\n",
    "#             [[13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[19, 20, 21],\n",
    "#              [22, 23, 24]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ea7fe74-0506-46dc-8387-b52189c2add5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "t10 = torch.tensor([1, 2, 3])  # 1차원 텐서, shape은 (3,)\n",
    "t11 = torch.tensor([4, 5, 6])  # 1차원 텐서, shape은 (3,)\n",
    "\n",
    "# t10과 t11을 수평으로 쌓아 (6,) 모양의 텐서 생성\n",
    "t12 = torch.hstack((t10, t11))\n",
    "print(t12)\n",
    "# >>> tensor([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "t13 = torch.tensor([[1], [2], [3]])  # 2차원 텐서, shape은 (3, 1)\n",
    "t14 = torch.tensor([[4], [5], [6]])  # 2차원 텐서, shape은 (3, 1)\n",
    "\n",
    "# t13과 t14를 수평으로 쌓아 (3, 2) 모양의 텐서 생성\n",
    "t15 = torch.hstack((t13, t14))\n",
    "print(t15)\n",
    "# >>> tensor([[1, 4],\n",
    "#             [2, 5],\n",
    "#             [3, 6]])\n",
    "\n",
    "# 3차원 텐서, shape은 (2, 2, 3)\n",
    "t16 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "# 3차원 텐서, shape은 (2, 2, 3)\n",
    "t17 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t17.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "# t16과 t17을 수평으로 쌓아 (2, 4, 3) 모양의 텐서 생성\n",
    "t18 = torch.hstack([t16, t17])\n",
    "print(t18.shape)\n",
    "# >>> (2, 4, 3)\n",
    "\n",
    "print(t18)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6],\n",
    "#              [13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12],\n",
    "#              [19, 20, 21],\n",
    "#              [22, 23, 24]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830523b4-10dc-4abb-befd-0d3ecc1a84e5",
   "metadata": {},
   "source": [
    "브로드캐스팅 개념을 통해 서로 다른 크기의 텐서 간 연산을 수행하는 방법을 설명한다.\n",
    "이 과정에서 각 텐서의 차원 맞춤을 자동으로 수행하여 효율적인 연산을 가능하게 한다.\n",
    "브로드캐스팅 기술은 딥러닝에서 데이터의 형상에 따라 계산을 유연하게 처리하는 데 필수적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bae33f-84d2-4b68-8d97-a56d568e7b5c",
   "metadata": {},
   "source": [
    "# 숙제 후기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f37b9aa-982a-4d3d-bdde-7725195244f9",
   "metadata": {},
   "source": [
    "물론 수업때도 잘 알려주셨지만, 과제를 통해 실습을 함으로써 추가적으로 더 잘 이해가 되었고 기억에 남는 것 같습니다. 텐서는 기본적으로 다차원 배열을 의미하며, 데이터의 저장과 연산을 효율적으로 수행할 수 있도록 설계된 구조이고, 딥러닝 모델 학습에 용이하다는 것을 알게 되었습니다. 또한 텐서를 활용하기 위한 파이썬 문법들도 익히게 되었습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
