{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe613152-9b22-4875-a618-bbe626827e92",
   "metadata": {},
   "source": [
    "# a_2d_image_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd2b3e-968e-4616-a313-f92f844aea9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T13:06:20.303886Z",
     "start_time": "2024-09-24T13:06:18.170440Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio.v2 as imageio  # 이미지 입출력을 위한 라이브러리\n",
    "import torch  # 텐서 연산을 위한 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e6502-6524-4694-a73d-5f982aa2c7a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T13:06:20.722006Z",
     "start_time": "2024-09-24T13:06:20.600052Z"
    }
   },
   "outputs": [],
   "source": [
    "# 이미지 파일을 읽어들임\n",
    "img_arr = imageio.imread(os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"a_image-dog\", \"bobby.jpg\"))\n",
    "print(type(img_arr))  # 이미지 데이터 타입 출력\n",
    "print(img_arr.shape)  # 이미지의 shape 출력\n",
    "print(img_arr.dtype)  # 이미지의 데이터 타입 출력\n",
    "\n",
    "# 이미지를 PyTorch 텐서로 변환\n",
    "img = torch.from_numpy(img_arr)\n",
    "# 채널 순서를 (높이, 너비, 채널)에서 (채널, 높이, 너비)로 변경\n",
    "out = img.permute(2, 0, 1)\n",
    "print(out.shape)  # 변환된 텐서 shape 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e93d5c-0ec8-4a56-8ae3-ca09fb50578d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T13:06:22.517197Z",
     "start_time": "2024-09-24T13:06:21.217247Z"
    }
   },
   "outputs": [],
   "source": [
    "# 고양이 이미지가 있는 데이터 디렉토리 설정\n",
    "data_dir = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"b_image-cats\")\n",
    "# PNG 이미지 파일 목록 추출\n",
    "filenames = [\n",
    "  name for name in os.listdir(data_dir) if os.path.splitext(name)[-1] == '.png'\n",
    "]\n",
    "print(filenames)  # 파일 이름 출력\n",
    "\n",
    "from PIL import Image  # 이미지를 열기 위한 PIL 라이브러리\n",
    "\n",
    "# 각 PNG 파일에 대해 이미지를 열고 shape, dtype 출력\n",
    "for i, filename in enumerate(filenames):\n",
    "  image = Image.open(os.path.join(data_dir, filename))  # 이미지 열기\n",
    "  image.show()  # 이미지 표시\n",
    "  img_arr = imageio.imread(os.path.join(data_dir, filename))  # 이미지 읽기\n",
    "  print(img_arr.shape)  # 이미지의 shape 출력\n",
    "  print(img_arr.dtype)  # 이미지의 데이터 타입 출력\n",
    "\n",
    "# 배치 사이즈 설정 및 빈 텐서 생성 (3장의 이미지, 3채널, 256x256 크기)\n",
    "batch_size = 3\n",
    "batch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.uint8)\n",
    "\n",
    "# 각 이미지를 텐서로 변환 후 배치에 저장\n",
    "for i, filename in enumerate(filenames):\n",
    "  img_arr = imageio.imread(os.path.join(data_dir, filename))  # 이미지 읽기\n",
    "  img_t = torch.from_numpy(img_arr)  # NumPy 배열을 텐서로 변환\n",
    "  img_t = img_t.permute(2, 0, 1)  # 채널 순서 변경 (채널, 높이, 너비)\n",
    "  batch[i] = img_t  # 배치에 이미지 저장\n",
    "\n",
    "print(batch.shape)  # 배치 텐서 shape 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db91617c-86e9-4a74-b10a-6e060669f95e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T13:06:22.870491Z",
     "start_time": "2024-09-24T13:06:22.791326Z"
    }
   },
   "outputs": [],
   "source": [
    "# 텐서를 float 타입으로 변환 후 정규화 (0-255 값을 0-1 범위로)\n",
    "batch = batch.float()  # 텐서 타입을 float로 변환\n",
    "batch /= 255.0  # 정규화\n",
    "print(batch.dtype)  # 정규화된 텐서 데이터 타입 출력\n",
    "print(batch.shape)  # 정규화된 배치 텐서의 shape 출력\n",
    "\n",
    "# 각 채널의 평균과 표준편차 계산 후 정규화\n",
    "n_channels = batch.shape[1]  # 채널 수 가져오기\n",
    "\n",
    "for c in range(n_channels):\n",
    "  mean = torch.mean(batch[:, c])  # 각 채널의 평균 계산\n",
    "  std = torch.std(batch[:, c])  # 각 채널의 표준편차 계산\n",
    "  print(mean, std)  # 평균과 표준편차 출력\n",
    "  # 정규화: 각 채널의 평균을 빼고 표준편차로 나눔\n",
    "  batch[:, c] = (batch[:, c] - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e4bce-d507-48f4-b67f-13c6a48fa20b",
   "metadata": {},
   "source": [
    "# b_3d_image_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc58604-0800-4266-99ae-8f176f219960",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T13:06:23.588928Z",
     "start_time": "2024-09-24T13:06:22.939020Z"
    }
   },
   "outputs": [],
   "source": [
    "# DICOM 이미지 파일이 있는 디렉토리 경로 설정\n",
    "dir_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"c_volumetric-dicom\", \"2-LUNG_3.0_B70f-04083\")\n",
    "# DICOM 볼륨 데이터를 읽어들임\n",
    "vol_array = imageio.volread(dir_path, format='DICOM')\n",
    "print(type(vol_array))   # DICOM 데이터를 NumPy 배열로 출력\n",
    "print(vol_array.shape)   # 볼륨 데이터의 shape 출력 (99, 512, 512): 슬라이스 개수와 이미지 크기\n",
    "print(vol_array.dtype)   # 볼륨 데이터의 dtype 출력 (int16)\n",
    "print(vol_array[0])  # 첫 번째 슬라이스 데이터 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c60b83-48c3-462e-b092-e014133ca509",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-24T13:06:23.621890Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # 볼륨 데이터를 시각화하기 위한 라이브러리\n",
    "\n",
    "# 99개의 슬라이스를 10x10 격자형으로 시각화\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for id in range(0, 99):\n",
    "  fig.add_subplot(10, 10, id + 1)  # 10x10 subplot 생성\n",
    "  plt.imshow(vol_array[id])  # 각 슬라이스 이미지 출력\n",
    "plt.show()  # 이미지를 화면에 표시\n",
    "\n",
    "import torch  # 텐서 연산을 위한 PyTorch\n",
    "\n",
    "# 볼륨 데이터를 텐서로 변환하고 float 타입으로 변경\n",
    "vol = torch.from_numpy(vol_array).float()\n",
    "vol = torch.unsqueeze(vol, 0)  # 채널 차원을 추가\n",
    "vol = torch.unsqueeze(vol, 0)  # 배치 차원을 추가\n",
    "\n",
    "print(vol.shape)  # 텐서의 shape 출력: torch.Size([1, 1, 99, 512, 512])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a645a466-0851-430d-b255-5c37b9f9528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 슬라이스 차원(3, 4)을 기준으로 평균과 표준편차 계산\n",
    "mean = torch.mean(vol, dim=(3, 4), keepdim=True)  # 각 슬라이스의 평균 계산\n",
    "print(mean.shape)  # 평균 텐서의 shape 출력: torch.Size([1, 1, 99, 1, 1])\n",
    "std = torch.std(vol, dim=(3, 4), keepdim=True)  # 각 슬라이스의 표준편차 계산\n",
    "print(std.shape)  # 표준편차 텐서의 shape 출력: torch.Size([1, 1, 99, 1, 1])\n",
    "\n",
    "# 볼륨 데이터를 정규화 (mean=0, std=1)\n",
    "vol = (vol - mean) / std\n",
    "print(vol.shape)  # 정규화된 텐서의 shape 출력: torch.Size([1, 1, 99, 512, 512])\n",
    "\n",
    "print(vol[0, 0, 0])  # 첫 번째 슬라이스의 정규화된 텐서 값 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d329335-5709-4c89-96e1-6e17237f32bc",
   "metadata": {},
   "source": [
    "# c_tabular_wine_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe749392-2c19-4802-bf4e-3d9e7da62d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17027ebf-9db6-4c79-b3a9-a20cfd42c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 와인 데이터 경로 설정 및 NumPy로 데이터 로드\n",
    "wine_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"d_tabular-wine\", \"winequality-white.csv\")\n",
    "wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=\";\", skiprows=1)  # CSV 데이터 읽기\n",
    "print(wineq_numpy.dtype)  # 데이터 타입 출력: float32\n",
    "print(wineq_numpy.shape)  # 데이터 shape 출력: (4898, 12)\n",
    "print(wineq_numpy)  # 데이터 출력\n",
    "print()\n",
    "\n",
    "# CSV 파일의 헤더(컬럼 이름) 추출\n",
    "col_list = next(csv.reader(open(wine_path), delimiter=';'))\n",
    "print(col_list)  # 컬럼 이름 리스트 출력\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f58d9c7-deb0-4757-9d45-07728f8af36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# NumPy 배열을 PyTorch 텐서로 변환\n",
    "wineq = torch.from_numpy(wineq_numpy)\n",
    "print(wineq.dtype)  # 텐서 데이터 타입 출력: torch.float32\n",
    "print(wineq.shape)  # 텐서 shape 출력: (4898, 12)\n",
    "print()\n",
    "\n",
    "# 데이터와 타겟 분리 (데이터는 마지막 열 제외, 타겟은 마지막 열)\n",
    "data = wineq[:, :-1]  # 마지막 열을 제외한 데이터 부분\n",
    "print(data.dtype)  # 데이터의 dtype 출력: torch.float32\n",
    "print(data.shape)  # 데이터의 shape 출력: (4898, 11)\n",
    "print(data)  # 데이터 출력\n",
    "print()\n",
    "\n",
    "target = wineq[:, -1]  # 마지막 열인 타겟 부분 선택\n",
    "print(target.dtype)  # 타겟의 dtype 출력: torch.float32\n",
    "print(target.shape)  # 타겟의 shape 출력: (4898,)\n",
    "print(target)  # 타겟 출력\n",
    "print()\n",
    "\n",
    "target = target.long()  # 타겟을 정수형으로 변환\n",
    "print(target.dtype)  # 변환된 타겟의 dtype 출력: torch.int64\n",
    "print(target.shape)  # 변환된 타겟의 shape 출력: (4898,)\n",
    "print(target)  # 변환된 타겟 출력\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0df9dd-3b1e-425a-a376-da207d896451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 타겟을 이용해 원-핫 인코딩 수행\n",
    "eye_matrix = torch.eye(10)  # 크기가 10인 단위행렬 생성 (one-hot 인코딩에 사용)\n",
    "onehot_target = eye_matrix[target]  # 타겟 값을 인덱스로 원-핫 벡터 생성\n",
    "\n",
    "print(onehot_target.shape)  # 원-핫 인코딩된 타겟의 shape 출력: torch.Size([4898, 10])\n",
    "print(onehot_target[0])  # 첫 번째 원-핫 벡터 출력\n",
    "print(onehot_target[1])  # 두 번째 원-핫 벡터 출력\n",
    "print(onehot_target[-2])  # 뒤에서 두 번째 원-핫 벡터 출력\n",
    "print(onehot_target)  # 전체 원-핫 인코딩된 타겟 출력\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd22b6-83f6-4f20-9869-9fccd157f972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터의 평균과 분산을 구하고 정규화 수행\n",
    "data_mean = torch.mean(data, dim=0)  # 각 피처의 평균 계산\n",
    "data_var = torch.var(data, dim=0)  # 각 피처의 분산 계산\n",
    "data = (data - data_mean) / torch.sqrt(data_var)  # 정규화 수행\n",
    "print(data)  # 정규화된 데이터 출력\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc54af27-179e-4c65-80c1-cf884e9841ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터셋을 훈련용/검증용으로 나누기\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, onehot_target, test_size=0.2)  # 데이터 분할\n",
    "\n",
    "print(X_train.shape)  # 훈련용 데이터 shape 출력: torch.Size([3918, 11])\n",
    "print(y_train.shape)  # 훈련용 타겟 shape 출력: torch.Size([3918, 10])\n",
    "print(X_test.shape)  # 검증용 데이터 shape 출력: torch.Size([980, 11])\n",
    "print(y_test.shape)  # 검증용 타겟 shape 출력: torch.Size([980, 10])\n",
    "print()\n",
    "\n",
    "# 와인 데이터를 로드하고 전처리하여 훈련/검증 데이터셋으로 분할하는 함수 정의\n",
    "def get_wine_data():\n",
    "  wine_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"d_tabular-wine\", \"winequality-white.csv\")\n",
    "  wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=\";\", skiprows=1)\n",
    "\n",
    "  wineq = torch.from_numpy(wineq_numpy)\n",
    "\n",
    "  data = wineq[:, :-1]  # 마지막 열을 제외한 데이터 부분\n",
    "  target = wineq[:, -1].long()  # 타겟을 정수형으로 변환\n",
    "\n",
    "  eye_matrix = torch.eye(10)\n",
    "  onehot_target = eye_matrix[target]\n",
    "\n",
    "  data_mean = torch.mean(data, dim=0)\n",
    "  data_var = torch.var(data, dim=0)\n",
    "  data = (data - data_mean) / torch.sqrt(data_var)\n",
    "\n",
    "  X_train, X_valid, y_train, y_valid = train_test_split(data, onehot_target, test_size=0.2)\n",
    "\n",
    "  return X_train, X_valid, y_train, y_valid  # 훈련용 및 검증용 데이터 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c6b116-9b5c-4d9c-9b79-67b34a277f38",
   "metadata": {},
   "source": [
    "# d_tabular_california_housing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f3db4-e65f-44a9-8501-bad89d7c7ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/implement-linear-regression-on-boston-housing-dataset-by-pytorch-c5d29546f938\n",
    "# https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset\n",
    "# 캘리포니아 주택 가격 데이터셋을 로드하고 전처리하는 코드입니다.\n",
    "import torch\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a52a91-8487-4c13-aed1-c645cd718c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 캘리포니아 주택 데이터셋 로드\n",
    "housing = fetch_california_housing()\n",
    "print(housing.keys())  # 데이터셋의 키 정보 출력\n",
    "\n",
    "print(type(housing.data))  # 데이터 타입 출력: <class 'numpy.ndarray'>\n",
    "print(housing.data.dtype)  # 데이터 타입 출력: float64\n",
    "print(housing.data.shape)  # 데이터의 shape 출력: (20640, 8)\n",
    "print(housing.feature_names)  # 특성 이름 출력\n",
    "\n",
    "print(housing.target.shape)  # 타겟의 shape 출력: (20640,)\n",
    "print(housing.target_names)  # 타겟 이름 출력: None (타겟 이름 없음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39283bd-860a-42e9-a80e-b4734d7d7dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 데이터의 최소값과 최대값 출력\n",
    "print(housing.data.min(), housing.data.max())  # 데이터의 범위\n",
    "\n",
    "# 데이터 평균 및 분산 계산 후 정규화\n",
    "data_mean = np.mean(housing.data, axis=0)  # 각 특성의 평균\n",
    "data_var = np.var(housing.data, axis=0)  # 각 특성의 분산\n",
    "data = (housing.data - data_mean) / np.sqrt(data_var)  # 정규화\n",
    "target = housing.target  # 타겟 값\n",
    "\n",
    "# 정규화된 데이터의 최소값과 최대값 출력\n",
    "print(data.min(), data.max())  # 정규화된 데이터의 범위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5734e333-22f9-47ee-8a2b-7078be42746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터셋을 훈련용 및 테스트용으로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\n",
    "\n",
    "# NumPy 배열을 PyTorch 텐서로 변환\n",
    "X_train = torch.from_numpy(X_train).float()  # 훈련 데이터\n",
    "X_test = torch.from_numpy(X_test).float()  # 테스트 데이터\n",
    "y_train = torch.from_numpy(y_train).float()  # 훈련 타겟\n",
    "y_test = torch.from_numpy(y_test).float()  # 테스트 타겟\n",
    "\n",
    "# 훈련 데이터와 타겟의 shape 출력\n",
    "print(X_train.shape)  # 훈련 데이터의 shape: torch.Size([16512, 8])\n",
    "print(y_train.shape)  # 훈련 타겟의 shape: torch.Size([16512])\n",
    "\n",
    "# 테스트 데이터와 타겟의 shape 출력\n",
    "print(X_test.shape)  # 테스트 데이터의 shape: torch.Size([4128, 8])\n",
    "print(y_test.shape)  # 테스트 타겟의 shape: torch.Size([4128])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b569d-e995-421b-8f44-1cb87784b64b",
   "metadata": {},
   "source": [
    "# e_bikes_sharing_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8d1b5-a943-4c84-88b3-df8fcb96bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6652fc52-deab-4d8a-a530-38547d1cbb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(edgeitems=2, threshold=50, linewidth=75)  # 출력 설정\n",
    "\n",
    "# 자전거 공유 데이터셋 경로 설정\n",
    "bikes_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"e_time-series-bike-sharing-dataset\", \"hour-fixed.csv\")\n",
    "\n",
    "# CSV 파일에서 데이터를 로드\n",
    "bikes_numpy = np.loadtxt(\n",
    "  fname=bikes_path, dtype=np.float32, delimiter=\",\", skiprows=1,\n",
    "  converters={\n",
    "    1: lambda x: float(x[8:10])  # 날짜에서 시간을 추출하여 변환\n",
    "  }\n",
    ")\n",
    "bikes = torch.from_numpy(bikes_numpy)  # NumPy 배열을 PyTorch 텐서로 변환\n",
    "print(bikes.shape)  # 전체 데이터의 shape 출력\n",
    "\n",
    "# 하루 단위로 데이터를 변환\n",
    "daily_bikes = bikes.view(-1, 24, bikes.shape[1])\n",
    "print(daily_bikes.shape)  # 하루 단위 데이터의 shape: torch.Size([730, 24, 17])\n",
    "\n",
    "# 데이터와 타겟 분리\n",
    "daily_bikes_data = daily_bikes[:, :, :-1]  # 마지막 열 제외 (타겟)\n",
    "daily_bikes_target = daily_bikes[:, :, -1].unsqueeze(dim=-1)  # 타겟 열 추가 차원 추가\n",
    "\n",
    "print(daily_bikes_data.shape)  # 데이터의 shape 출력\n",
    "print(daily_bikes_target.shape)  # 타겟의 shape 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393af92c-08e5-4370-a554-5e8ae613aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_day_data = daily_bikes_data[0]  # 첫 날 데이터\n",
    "print(first_day_data.shape)  # 첫 날 데이터의 shape 출력\n",
    "\n",
    "# 날씨 상황을 long 타입으로 변환하여 출력\n",
    "# 1: 맑음, 2: 안개, 3: 가벼운 비/눈, 4: 강한 비/눈\n",
    "print(first_day_data[:, 9].long())  # 날씨 열 출력\n",
    "eye_matrix = torch.eye(4)  # 단위 행렬 생성\n",
    "print(eye_matrix)  # 단위 행렬 출력\n",
    "\n",
    "# 날씨 정보를 one-hot 인코딩\n",
    "weather_onehot = eye_matrix[first_day_data[:, 9].long() - 1]\n",
    "print(weather_onehot.shape)  # 원-핫 인코딩된 날씨 정보의 shape 출력\n",
    "print(weather_onehot)  # 원-핫 인코딩된 날씨 정보 출력\n",
    "\n",
    "# 첫 날 데이터에 날씨 정보를 추가\n",
    "first_day_data_torch = torch.cat(tensors=(first_day_data, weather_onehot), dim=1)\n",
    "print(first_day_data_torch.shape)  # 결합된 데이터의 shape 출력\n",
    "print(first_day_data_torch)  # 결합된 데이터 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04ab70-e513-4e51-ab03-b2c251b95750",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_data_torch_list = []  # 일일 데이터 리스트 초기화\n",
    "\n",
    "# 각 일별로 데이터를 처리\n",
    "for daily_idx in range(daily_bikes_data.shape[0]):  # 730일 반복\n",
    "  day = daily_bikes_data[daily_idx]  # 하루 데이터\n",
    "  weather_onehot = eye_matrix[day[:, 9].long() - 1]  # 날씨 정보 원-핫 인코딩\n",
    "  day_data_torch = torch.cat(tensors=(day, weather_onehot), dim=1)  # 데이터 결합\n",
    "  day_data_torch_list.append(day_data_torch)  # 리스트에 추가\n",
    "\n",
    "print(len(day_data_torch_list))  # 리스트 길이 출력\n",
    "daily_bikes_data = torch.stack(day_data_torch_list, dim=0)  # 리스트를 텐서로 변환\n",
    "print(daily_bikes_data.shape)  # 최종 데이터의 shape 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d10ee21-daeb-4a1a-821d-10836ffc2e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 열들을 선택하여 shape 출력\n",
    "print(daily_bikes_data[:, :, :9].shape, daily_bikes_data[:, :, 10:].shape)\n",
    "# 'instant'와 'whethersit' 열을 제거하고 결합\n",
    "daily_bikes_data = torch.cat(\n",
    "  [daily_bikes_data[:, :, 1:9], daily_bikes_data[:, :, 10:]], dim=2\n",
    ")  \n",
    "print(daily_bikes_data.shape)  # 최종 데이터의 shape 출력\n",
    "\n",
    "# 온도 데이터 정규화\n",
    "temperatures = daily_bikes_data[:, :, 8]  # 온도 열 선택\n",
    "daily_bikes_data[:, :, 8] = (daily_bikes_data[:, :, 8] - torch.mean(temperatures)) / torch.std(temperatures)  # 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e314db9-e961-4460-acea-258be9af5be3",
   "metadata": {},
   "source": [
    "# f_hourly_bikes_sharing_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f859e-29ab-4000-b63c-499d4cd537a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08770b7c-a60f-4461-8252-f52a736a6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = str(Path(os.getcwd()).resolve().parent.parent)\n",
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "torch.set_printoptions(edgeitems=2, threshold=50, linewidth=75)\n",
    "\n",
    "# 데이터 파일 경로 설정\n",
    "bikes_path = os.path.join(BASE_PATH, \"_00_data\", \"e_time-series-bike-sharing-dataset\", \"hour-fixed.csv\")\n",
    "\n",
    "# CSV 파일에서 데이터를 읽어 numpy 배열로 변환\n",
    "bikes_numpy = np.loadtxt(\n",
    "    fname=bikes_path, dtype=np.float32, delimiter=\",\", skiprows=1,\n",
    "    converters={\n",
    "        1: lambda x: float(x[8:10])  # 2011-01-07 --> 07 --> 7\n",
    "    }\n",
    ")\n",
    "\n",
    "# numpy 배열을 PyTorch 텐서로 변환\n",
    "bikes_data = torch.from_numpy(bikes_numpy).to(torch.float)\n",
    "print(bikes_data.shape)    # >>> torch.Size([17520, 17])\n",
    "\n",
    "# 타겟과 피쳐 분리\n",
    "bikes_target = bikes_data[:, -1].unsqueeze(dim=-1)  # 'cnt'\n",
    "bikes_data = bikes_data[:, :-1]   # >>> torch.Size([17520, 16])\n",
    "\n",
    "# 날씨 데이터를 위한 단위 행렬 생성\n",
    "eye_matrix = torch.eye(4)\n",
    "\n",
    "data_torch_list = []\n",
    "for idx in range(bikes_data.shape[0]):  # range(730)\n",
    "    hour_data = bikes_data[idx]  # hour_data.shape: [17]\n",
    "    weather_onehot = eye_matrix[hour_data[9].long() - 1]  # 날씨에 대한 원-핫 인코딩\n",
    "    concat_data_torch = torch.cat(tensors=(hour_data, weather_onehot), dim=-1)\n",
    "    # concat_data_torch.shape: [20]\n",
    "    data_torch_list.append(concat_data_torch)\n",
    "\n",
    "# 데이터를 스택하여 최종 데이터셋 생성\n",
    "bikes_data = torch.stack(data_torch_list, dim=0)\n",
    "bikes_data = torch.cat([bikes_data[:, 1:9], bikes_data[:, 10:]], dim=-1)  # 'instant' 및 'whethersit' 열 삭제\n",
    "\n",
    "print(bikes_data.shape)\n",
    "print(bikes_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e94636-114b-494d-8165-9e9b53d0860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시퀀스 크기 및 데이터 크기 설정\n",
    "sequence_size = 24\n",
    "validation_size = 96\n",
    "test_size = 24\n",
    "y_normalizer = 100\n",
    "\n",
    "data_size = len(bikes_data) - sequence_size + 1\n",
    "print(\"data_size: {0}\".format(data_size))\n",
    "train_size = data_size - (validation_size + test_size)\n",
    "print(\"train_size: {0}, validation_size: {1}, test_size: {2}\".format(train_size, validation_size, test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f98864-c13f-4135-8e94-2c9ca784f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 생성\n",
    "row_cursor = 0\n",
    "\n",
    "X_train_list = []\n",
    "y_train_regression_list = []\n",
    "for idx in range(0, train_size):\n",
    "  sequence_data = bikes_data[idx: idx + sequence_size]  # 시퀀스 데이터\n",
    "  sequence_target = bikes_target[idx + sequence_size - 1]  # 시퀀스 타겟\n",
    "  X_train_list.append(sequence_data)\n",
    "  y_train_regression_list.append(sequence_target)\n",
    "  row_cursor += 1\n",
    "\n",
    "X_train = torch.stack(X_train_list, dim=0).to(torch.float)  # 훈련 데이터 텐서로 변환\n",
    "y_train_regression = torch.tensor(y_train_regression_list, dtype=torch.float32) / y_normalizer  # 타겟 정규화\n",
    "\n",
    "# 데이터 정규화\n",
    "m = X_train.mean(dim=0, keepdim=True)\n",
    "s = X_train.std(dim=0, keepdim=True)\n",
    "X_train = (X_train - m) / s\n",
    "\n",
    "print(X_train.shape, y_train_regression.shape)\n",
    "# >>> torch.Size([17376, 24, 19]) torch.Size([17376])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14b1e3f-09ce-41a4-9de3-57c30beab298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 데이터셋 생성\n",
    "X_validation_list = []\n",
    "y_validation_regression_list = []\n",
    "for idx in range(row_cursor, row_cursor + validation_size):\n",
    "    sequence_data = bikes_data[idx: idx + sequence_size]  # 시퀀스 데이터\n",
    "    sequence_target = bikes_target[idx + sequence_size - 1]  # 해당 시퀀스의 타겟\n",
    "    X_validation_list.append(sequence_data)\n",
    "    y_validation_regression_list.append(sequence_target)\n",
    "    row_cursor += 1\n",
    "\n",
    "# 검증 데이터 텐서로 변환\n",
    "X_validation = torch.stack(X_validation_list, dim=0).to(torch.float)\n",
    "y_validation_regression = torch.tensor(y_validation_regression_list, dtype=torch.float32) / y_normalizer\n",
    "\n",
    "# 데이터 정규화\n",
    "X_validation = (X_validation - m) / s\n",
    "\n",
    "print(X_validation.shape, y_validation_regression.shape)\n",
    "# >>> torch.Size([96, 24, 19]) torch.Size([96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa4fa67-213d-4d63-b72b-6e83bcd9c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 생성\n",
    "X_test_list = []\n",
    "y_test_regression_list = []\n",
    "for idx in range(row_cursor, row_cursor + test_size):\n",
    "    sequence_data = bikes_data[idx: idx + sequence_size]  # 시퀀스 데이터\n",
    "    sequence_target = bikes_target[idx + sequence_size - 1]  # 해당 시퀀스의 타겟\n",
    "    X_test_list.append(sequence_data)\n",
    "    y_test_regression_list.append(sequence_target)\n",
    "    row_cursor += 1\n",
    "\n",
    "# 테스트 데이터 텐서로 변환\n",
    "X_test = torch.stack(X_test_list, dim=0).to(torch.float)\n",
    "y_test_regression = torch.tensor(y_test_regression_list, dtype=torch.float32) / y_normalizer\n",
    "\n",
    "# 테스트 데이터 정규화\n",
    "X_test = (X_test - m) / s  # 정규화를 올바르게 적용\n",
    "\n",
    "print(X_test.shape, y_test_regression.shape)\n",
    "# >>> torch.Size([24, 24, 18]) torch.Size([24])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7577c11e-b1b7-491e-81fa-38504f7be741",
   "metadata": {},
   "source": [
    "# g_cryptocurrency_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb3816-6248-4868-b47d-b61da6faab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89097ca1-0075-4652-99b5-2fc7a12c333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 경로 설정\n",
    "BASE_PATH = str(Path(os.getcwd()).resolve().parent.parent)\n",
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "# 데이터 경로 설정\n",
    "btc_krw_path = os.path.join(BASE_PATH, \"_00_data\", \"k_cryptocurrency\", \"BTC_KRW.csv\")\n",
    "df = pd.read_csv(btc_krw_path)  # CSV 파일에서 데이터 읽기\n",
    "print(df)\n",
    "\n",
    "# 데이터 크기 및 열 정보 출력\n",
    "row_size = len(df)\n",
    "print(\"row_size:\", row_size)\n",
    "columns = df.columns  # ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "print([column for column in columns])\n",
    "date_list = df['Date']  # 날짜 정보 저장\n",
    "df = df.drop(columns=['Date'])  # 날짜 열 제거\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394dfb97-6f1a-420a-b420-a3a459a82a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "sequence_size = 10  # 시퀀스 크기\n",
    "validation_size = 100  # 검증 데이터 크기\n",
    "test_size = 50  # 테스트 데이터 크기\n",
    "\n",
    "# 데이터 크기 계산\n",
    "data_size = row_size - sequence_size + 1\n",
    "print(\"data_size: {0}\".format(data_size))\n",
    "train_size = data_size - (validation_size + test_size)  # 훈련 데이터 크기 계산\n",
    "print(\"train_size: {0}, validation_size: {1}, test_size: {2}\".format(train_size, validation_size, test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc721015-c29f-4892-9111-bf7558441197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 준비\n",
    "row_cursor = 0\n",
    "y_normalizer = 1.0e7  # 가격 정규화 값\n",
    "\n",
    "X_train_list = []\n",
    "y_train_regression_list = []\n",
    "y_train_classification_list = []\n",
    "y_train_date = []\n",
    "\n",
    "for idx in range(0, train_size):\n",
    "    # 시퀀스 데이터 생성\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values  # 시퀀스 데이터\n",
    "    X_train_list.append(torch.from_numpy(sequence_data))  # 텐서로 변환\n",
    "    y_train_regression_list.append(df.iloc[idx + sequence_size - 1][\"Close\"])  # 회귀 레이블\n",
    "    y_train_classification_list.append(\n",
    "        1 if df.iloc[idx + sequence_size - 1][\"Close\"] >= df.iloc[idx + sequence_size - 2][\"Close\"] else 0  # 분류 레이블\n",
    "    )\n",
    "    y_train_date.append(date_list[idx + sequence_size - 1])  # 날짜 저장\n",
    "    row_cursor += 1\n",
    "\n",
    "# 텐서로 변환 및 정규화\n",
    "X_train = torch.stack(X_train_list, dim=0).to(torch.float)\n",
    "y_train_regression = torch.tensor(y_train_regression_list, dtype=torch.float32) / y_normalizer\n",
    "y_train_classification = torch.tensor(y_train_classification_list, dtype=torch.int64)\n",
    "\n",
    "m = X_train.mean(dim=0, keepdim=True)  # 평균\n",
    "s = X_train.std(dim=0, keepdim=True)  # 표준편차\n",
    "X_train -= m  # 평균으로 정규화\n",
    "X_train /= s  # 표준편차로 정규화\n",
    "\n",
    "print(X_train.shape, y_train_regression.shape, y_train_classification.shape)\n",
    "print(\"Label - Start Date: {0} ~ End Date: {1}\".format(y_train_date[0], y_train_date[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12d4a3a-090b-40e1-9eb3-6ca4b2a72f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 데이터 준비\n",
    "X_validation_list = []\n",
    "y_validation_regression_list = []\n",
    "y_validation_classification_list = []\n",
    "y_validation_date = []\n",
    "\n",
    "for idx in range(row_cursor, row_cursor + validation_size):\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values  # 시퀀스 데이터\n",
    "    X_validation_list.append(torch.from_numpy(sequence_data))  # 텐서로 변환\n",
    "    y_validation_regression_list.append(df.iloc[idx + sequence_size - 1][\"Close\"])  # 회귀 레이블\n",
    "    y_validation_classification_list.append(\n",
    "        1 if df.iloc[idx + sequence_size - 1][\"Close\"] >= df.iloc[idx + sequence_size - 2][\"Close\"] else 0  # 분류 레이블\n",
    "    )\n",
    "    y_validation_date.append(date_list[idx + sequence_size - 1])  # 날짜 저장\n",
    "    row_cursor += 1\n",
    "\n",
    "# 텐서로 변환 및 정규화\n",
    "X_validation = torch.stack(X_validation_list, dim=0).to(torch.float)\n",
    "y_validation_regression = torch.tensor(y_validation_regression_list, dtype=torch.float32) / y_normalizer\n",
    "y_validation_classification = torch.tensor(y_validation_classification_list, dtype=torch.int64)\n",
    "\n",
    "X_validation = (X_validation - m) / s  # 정규화\n",
    "\n",
    "print(X_validation.shape, y_validation_regression.shape, y_validation_classification.shape)\n",
    "print(\"Label - Start Date: {0} ~ End Date: {1}\".format(y_validation_date[0], y_validation_date[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178990d-972d-4492-805d-29787e334910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 준비\n",
    "X_test_list = []\n",
    "y_test_regression_list = []\n",
    "y_test_classification_list = []\n",
    "y_test_date = []\n",
    "\n",
    "for idx in range(row_cursor, row_cursor + test_size):\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values  # 시퀀스 데이터\n",
    "    X_test_list.append(torch.from_numpy(sequence_data))  # 텐서로 변환\n",
    "    y_test_regression_list.append(df.iloc[idx + sequence_size - 1][\"Close\"])  # 회귀 레이블\n",
    "    y_test_classification_list.append(\n",
    "        1 if df.iloc[idx + sequence_size - 1][\"Close\"] > df.iloc[idx + sequence_size - 2][\"Close\"] else 0  # 분류 레이블\n",
    "    )\n",
    "    y_test_date.append(date_list[idx + sequence_size - 1])  # 날짜 저장\n",
    "    row_cursor += 1\n",
    "\n",
    "# 텐서로 변환 및 정규화\n",
    "X_test = torch.stack(X_test_list, dim=0).to(torch.float)\n",
    "y_test_regression = torch.tensor(y_test_regression_list, dtype=torch.float32) / y_normalizer\n",
    "y_test_classification = torch.tensor(y_test_classification_list, dtype=torch.int64)\n",
    "\n",
    "X_test = (X_test - m) / s  # 정규화\n",
    "\n",
    "print(X_test.shape, y_test_regression.shape, y_test_classification.shape)\n",
    "print(\"Label - Start Date: {0} ~ End Date: {1}\".format(y_test_date[0], y_test_date[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c033d8-da3a-4952-b9f1-9269ad25ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 시각화\n",
    "fig, ax = plt.subplots(1, figsize=(13, 7))\n",
    "ax.plot(y_train_date, y_train_regression * y_normalizer, label=\"y_train_regression\", linewidth=2)\n",
    "ax.plot(y_validation_date, y_validation_regression * y_normalizer, label=\"y_validation\", linewidth=2)\n",
    "ax.plot(y_test_date, y_test_regression * y_normalizer, label=\"y_test\", linewidth=2)\n",
    "ax.set_ylabel('Bitcoin [KRW]', fontsize=14)\n",
    "ax.set_xticks(ax.get_xticks()[::200])  # X축 레이블 조정\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050265ad-36af-43f8-ba52-8f8dd6b1cafc",
   "metadata": {},
   "source": [
    "# h_audio_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec91187-59c2-4198-9d9a-1bb928e5bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import scipy.io.wavfile as wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4556d69-9a42-490e-8fe0-b8eb9331d9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오디오 파일 경로 설정\n",
    "audio_1_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"f_audio-chirp\", \"1-100038-A-14.wav\")\n",
    "audio_2_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"f_audio-chirp\", \"1-100210-A-36.wav\")\n",
    "\n",
    "# WAV 파일을 읽어 샘플링 주파수와 파형 배열을 반환\n",
    "freq_1, waveform_arr_1 = wavfile.read(audio_1_path)\n",
    "print(freq_1)\n",
    "print(type(waveform_arr_1))\n",
    "print(len(waveform_arr_1))\n",
    "print(waveform_arr_1)\n",
    "\n",
    "# 두 번째 WAV 파일을 읽어 샘플링 주파수와 파형 배열을 반환\n",
    "freq_2, waveform_arr_2 = wavfile.read(audio_2_path)\n",
    "\n",
    "# 2개의 오디오 파일의 파형을 텐서 형태로 저장\n",
    "waveform = torch.empty(2, 1, 220_500)  # 텐서 초기화\n",
    "waveform[0, 0] = torch.from_numpy(waveform_arr_1).float()  # 첫 번째 오디오 파형\n",
    "waveform[1, 0] = torch.from_numpy(waveform_arr_2).float()  # 두 번째 오디오 파형\n",
    "print(waveform.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ff1727-9be7-4ee9-b8b8-f2c485e22584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "# 입력 신호의 스펙트로그램을 계산\n",
    "_, _, sp_arr_1 = signal.spectrogram(waveform_arr_1, freq_1)\n",
    "_, _, sp_arr_2 = signal.spectrogram(waveform_arr_2, freq_2)\n",
    "\n",
    "# NumPy 배열을 PyTorch 텐서로 변환\n",
    "sp_1 = torch.from_numpy(sp_arr_1)\n",
    "sp_2 = torch.from_numpy(sp_arr_2)\n",
    "print(sp_1.shape)\n",
    "print(sp_2.shape)\n",
    "\n",
    "# 스펙트로그램을 텐서로 변환\n",
    "sp_left_t = torch.from_numpy(sp_arr_1)\n",
    "sp_right_t = torch.from_numpy(sp_arr_2)\n",
    "print(sp_left_t.shape)\n",
    "print(sp_right_t.shape)\n",
    "\n",
    "# 두 개의 스펙트로그램을 스택하여 3차원 텐서 생성\n",
    "sp_t = torch.stack((sp_left_t, sp_right_t), dim=0).unsqueeze(dim=0)  # 새로운 차원 추가\n",
    "print(sp_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d4b423-cec8-49b4-81a1-2362d44fe4c1",
   "metadata": {},
   "source": [
    "# i_video_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5067b52-0c69-46e3-b690-02b4f0589a5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import imageio\n",
    "\n",
    "# 비디오 파일 경로 설정 (상위 디렉터리로부터 \"_00_data/g_video-cockatoo/cockatoo.mp4\" 경로로 설정)\n",
    "video_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"g_video-cockatoo\", \"cockatoo.mp4\")\n",
    "\n",
    "# 비디오 파일 리더(reader) 생성\n",
    "reader = imageio.get_reader(video_path)\n",
    "\n",
    "# reader의 타입 출력\n",
    "print(type(reader))\n",
    "\n",
    "# 비디오 메타데이터 가져오기 (프레임 수, 해상도 등)\n",
    "meta = reader.get_meta_data()\n",
    "print(meta)\n",
    "\n",
    "# 비디오의 각 프레임을 하나씩 가져와 torch 텐서로 변환\n",
    "for i, frame in enumerate(reader):\n",
    "  # 각 프레임을 NumPy 배열에서 torch 텐서로 변환하고 float 타입으로 캐스팅\n",
    "  frame = torch.from_numpy(frame).float()  # frame.shape: [360, 480, 3] (360x480 해상도, 3개의 채널)\n",
    "  print(i, frame.shape)   # 현재 프레임 번호와 텐서의 크기 출력\n",
    "\n",
    "# 비디오의 채널 수와 프레임 수를 설정 (3채널, 총 529프레임)\n",
    "n_channels = 3\n",
    "n_frames = 529\n",
    "\n",
    "# 빈 텐서 생성: (1, 529, 3, 480, 360) 형태로 비디오 데이터를 저장할 공간 할당\n",
    "video = torch.empty(1, n_frames, n_channels, *meta['size'])  # (1, 529, 3, 480, 360)\n",
    "print(video.shape)\n",
    "\n",
    "# 다시 프레임을 하나씩 읽어와서 비디오 텐서에 저장\n",
    "for i, frame in enumerate(reader):\n",
    "  # NumPy 배열을 torch 텐서로 변환하고 float 타입으로 변경\n",
    "  frame = torch.from_numpy(frame).float()       # frame.shape: [360, 480, 3]\n",
    "  # 차원 순서를 변경 (기본적으로 [높이, 너비, 채널] 순서인 배열을 [채널, 너비, 높이]로 변환)\n",
    "  frame = torch.permute(frame, dims=(2, 1, 0))  # frame.shape: [3, 480, 360]\n",
    "  # 변환한 프레임을 비디오 텐서의 해당 위치에 저장\n",
    "  video[0, i] = frame\n",
    "\n",
    "# 비디오 텐서의 차원 순서를 변경: (1, 3, 529, 480, 360) -> (1, 3채널, 529프레임, 480, 360) 형태\n",
    "video = video.permute(dims=(0, 2, 1, 3, 4))\n",
    "print(video.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b4f37f-d317-4c1c-813a-d43c1ff23f7f",
   "metadata": {},
   "source": [
    "# j_linear_regression_dataset_dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309e58b-5041-432c-ba32-19b7f930cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "class LinearRegressionDataset(Dataset):\n",
    "    def __init__(self, N=50, m=-3, b=2, *args, **kwargs):\n",
    "        # N: 샘플 수 (예: 50)\n",
    "        # m: 기울기\n",
    "        # b: 절편\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # N개의 랜덤 입력값 생성 (2차원)\n",
    "        self.x = torch.rand(N, 2)\n",
    "        # 잡음 추가 (0.2까지의 랜덤 값)\n",
    "        self.noise = torch.rand(N) * 0.2\n",
    "        self.m = m\n",
    "        self.b = b\n",
    "        # y 값 계산: y = mx + b + 잡음\n",
    "        self.y = (torch.sum(self.x * self.m) + self.b + self.noise).unsqueeze(-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 샘플 수 반환\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스의 샘플 (x, y) 반환\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __str__(self):\n",
    "        # 데이터셋의 정보 문자열 반환\n",
    "        str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "            len(self.x), self.x.shape, self.y.shape\n",
    "        )\n",
    "        return str\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # LinearRegressionDataset 인스턴스 생성\n",
    "    linear_regression_dataset = LinearRegressionDataset()\n",
    "\n",
    "    # 데이터셋 정보 출력\n",
    "    print(linear_regression_dataset)\n",
    "\n",
    "    print(\"#\" * 50, 1)\n",
    "\n",
    "    # 데이터셋의 각 샘플 출력\n",
    "    for idx, sample in enumerate(linear_regression_dataset):\n",
    "        input, target = sample\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input, target))\n",
    "\n",
    "    # 데이터셋을 훈련, 검증, 테스트 세트로 분할\n",
    "    train_dataset, validation_dataset, test_dataset = random_split(linear_regression_dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "    print(\"#\" * 50, 2)\n",
    "\n",
    "    # 각 데이터셋의 샘플 수 출력\n",
    "    print(len(train_dataset), len(validation_dataset), len(test_dataset))\n",
    "\n",
    "    print(\"#\" * 50, 3)\n",
    "\n",
    "    # 훈련 데이터 로더 생성 (배치 사이즈 4, 섞기 활성화)\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # 데이터 로더의 각 배치 출력\n",
    "    for idx, batch in enumerate(train_data_loader):\n",
    "        input, target = batch\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input, target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f9d3cf-3bb0-44ee-9d8e-b577556bc7fc",
   "metadata": {},
   "source": [
    "# k_2d_image_dataset_dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dec547-4dd4-4340-88d3-6ec13056c763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class DogCat2DImageDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # 이미지 전처리 변환: 크기 조정 및 텐서 변환\n",
    "        self.image_transforms = transforms.Compose([\n",
    "            transforms.Resize(size=(256, 256)),  # 이미지 크기를 256x256으로 조정\n",
    "            transforms.ToTensor()  # 이미지를 텐서로 변환\n",
    "        ])\n",
    "\n",
    "        # 개와 고양이 이미지가 저장된 디렉토리 경로 설정\n",
    "        dogs_dir = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"a_image-dog\")\n",
    "        cats_dir = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"b_image-cats\")\n",
    "\n",
    "        # 이미지 파일 리스트 생성\n",
    "        image_lst = [\n",
    "            Image.open(os.path.join(dogs_dir, \"bobby.jpg\")),  # (1280, 720, 3)\n",
    "            Image.open(os.path.join(cats_dir, \"cat1.png\")),  # (256, 256, 3)\n",
    "            Image.open(os.path.join(cats_dir, \"cat2.png\")),  # (256, 256, 3)\n",
    "            Image.open(os.path.join(cats_dir, \"cat3.png\"))   # (256, 256, 3)\n",
    "        ]\n",
    "\n",
    "        # 이미지를 전처리 및 텐서로 변환\n",
    "        image_lst = [self.image_transforms(img) for img in image_lst]\n",
    "        self.images = torch.stack(image_lst, dim=0)  # 변환된 이미지를 하나의 텐서로 쌓음\n",
    "\n",
    "        # 0: \"dog\", 1: \"cat\"\n",
    "        self.image_labels = torch.tensor([[0], [1], [1], [1]])  # 각 이미지에 대한 레이블 설정\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 총 샘플 수 반환\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스의 이미지와 레이블 반환\n",
    "        return self.images[idx], self.image_labels[idx]\n",
    "\n",
    "    def __str__(self):\n",
    "        # 데이터셋의 정보 문자열 반환\n",
    "        str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "            len(self.images), self.images.shape, self.image_labels.shape\n",
    "        )\n",
    "        return str\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # DogCat2DImageDataset 인스턴스 생성\n",
    "    dog_cat_2d_image_dataset = DogCat2DImageDataset()\n",
    "\n",
    "    # 데이터셋 정보 출력\n",
    "    print(dog_cat_2d_image_dataset)\n",
    "\n",
    "    print(\"#\" * 50, 1)\n",
    "\n",
    "    # 데이터셋의 각 샘플 출력\n",
    "    for idx, sample in enumerate(dog_cat_2d_image_dataset):\n",
    "        input, target = sample\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target))\n",
    "\n",
    "    # 데이터셋을 훈련과 테스트 세트로 분할\n",
    "    train_dataset, test_dataset = random_split(dog_cat_2d_image_dataset, [0.7, 0.3])\n",
    "\n",
    "    print(\"#\" * 50, 2)\n",
    "\n",
    "    # 각 데이터셋의 샘플 수 출력\n",
    "    print(len(train_dataset), len(test_dataset))\n",
    "\n",
    "    print(\"#\" * 50, 3)\n",
    "\n",
    "    # 훈련 데이터 로더 생성 (배치 사이즈 2, 섞기 활성화)\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # 데이터 로더의 각 배치 출력\n",
    "    for idx, batch in enumerate(train_data_loader):\n",
    "        input, target = batch\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e9622-894f-4bf8-9480-9d969489d389",
   "metadata": {},
   "source": [
    "# l_wine_dataset_dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aaddc5-f1a5-4c2b-b043-e64761a88917",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # CSV 파일 경로 설정\n",
    "        wine_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"d_tabular-wine\", \"winequality-white.csv\")\n",
    "        \n",
    "        # CSV 파일을 NumPy 배열로 로드 (첫 번째 행은 헤더이므로 건너뜀)\n",
    "        wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=\";\", skiprows=1)\n",
    "        wineq = torch.from_numpy(wineq_numpy)  # NumPy 배열을 PyTorch 텐서로 변환\n",
    "\n",
    "        # 데이터와 레이블 분리\n",
    "        data = wineq[:, :-1]  # 모든 행과 마지막 열을 제외한 모든 열 선택\n",
    "        data_mean = torch.mean(data, dim=0)  # 각 특성의 평균 계산\n",
    "        data_var = torch.var(data, dim=0)  # 각 특성의 분산 계산\n",
    "        self.data = (data - data_mean) / torch.sqrt(data_var)  # 정규화\n",
    "\n",
    "        target = wineq[:, -1].long()  # 레이블을 정수형으로 변환\n",
    "        eye_matrix = torch.eye(10)  # 10개의 클래스에 대한 단위 행렬 생성\n",
    "        self.target = eye_matrix[target]  # 레이블을 원-핫 인코딩으로 변환\n",
    "\n",
    "        # 데이터와 레이블의 길이가 같은지 확인\n",
    "        assert len(self.data) == len(self.target)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 총 샘플 수 반환\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스의 데이터와 레이블 반환\n",
    "        wine_feature = self.data[idx]\n",
    "        wine_target = self.target[idx]\n",
    "        return wine_feature, wine_target\n",
    "\n",
    "    def __str__(self):\n",
    "        # 데이터셋의 정보 문자열 반환\n",
    "        str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "            len(self.data), self.data.shape, self.target.shape\n",
    "        )\n",
    "        return str\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # WineDataset 인스턴스 생성\n",
    "    wine_dataset = WineDataset()\n",
    "\n",
    "    # 데이터셋 정보 출력\n",
    "    print(wine_dataset)\n",
    "\n",
    "    print(\"#\" * 50, 1)\n",
    "\n",
    "    # 데이터셋의 각 샘플 출력\n",
    "    for idx, sample in enumerate(wine_dataset):\n",
    "        input, target = sample\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "    # 데이터셋을 훈련, 검증, 테스트 세트로 분할\n",
    "    train_dataset, validation_dataset, test_dataset = random_split(wine_dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "    print(\"#\" * 50, 2)\n",
    "\n",
    "    # 각 데이터셋의 샘플 수 출력\n",
    "    print(len(train_dataset), len(validation_dataset), len(test_dataset))\n",
    "\n",
    "    print(\"#\" * 50, 3)\n",
    "\n",
    "    # 훈련 데이터 로더 생성 (배치 사이즈 32, 섞기 활성화, 마지막 배치 버리기)\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        drop_last=True  # 마지막 배치가 배치 크기보다 작으면 버림\n",
    "    )\n",
    "\n",
    "    # 데이터 로더의 각 배치 출력\n",
    "    for idx, batch in enumerate(train_data_loader):\n",
    "        input, target = batch\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44285cf6-83ce-4973-bcbc-8cfae2cdc017",
   "metadata": {},
   "source": [
    "# m_california_housing_dataset_dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa498d-9251-4371-8fbf-d9c59d67ff61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "class CaliforniaHousingDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # 사이킷런에서 캘리포니아 주택 데이터셋 로드\n",
    "        from sklearn.datasets import fetch_california_housing\n",
    "        housing = fetch_california_housing()\n",
    "        \n",
    "        # 데이터 평균과 분산 계산\n",
    "        data_mean = np.mean(housing.data, axis=0)\n",
    "        data_var = np.var(housing.data, axis=0)\n",
    "        \n",
    "        # 데이터 정규화 및 PyTorch 텐서로 변환\n",
    "        self.data = torch.tensor((housing.data - data_mean) / np.sqrt(data_var), dtype=torch.float32)\n",
    "        \n",
    "        # 타겟 값도 PyTorch 텐서로 변환 (차원 추가)\n",
    "        self.target = torch.tensor(housing.target, dtype=torch.float32).unsqueeze(dim=-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 총 샘플 수 반환\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스의 데이터와 타겟 반환\n",
    "        sample_data = self.data[idx]\n",
    "        sample_target = self.target[idx]\n",
    "        return sample_data, sample_target\n",
    "\n",
    "    def __str__(self):\n",
    "        # 데이터셋의 정보 문자열 반환\n",
    "        str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "            len(self.data), self.data.shape, self.target.shape\n",
    "        )\n",
    "        return str\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # CaliforniaHousingDataset 인스턴스 생성\n",
    "    california_housing_dataset = CaliforniaHousingDataset()\n",
    "\n",
    "    # 데이터셋 정보 출력\n",
    "    print(california_housing_dataset)\n",
    "\n",
    "    print(\"#\" * 50, 1)\n",
    "\n",
    "    # 데이터셋의 각 샘플 출력\n",
    "    for idx, sample in enumerate(california_housing_dataset):\n",
    "        input, target = sample\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "    # 데이터셋을 훈련, 검증, 테스트 세트로 분할\n",
    "    train_dataset, validation_dataset, test_dataset = random_split(california_housing_dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "    print(\"#\" * 50, 2)\n",
    "\n",
    "    # 각 데이터셋의 샘플 수 출력\n",
    "    print(len(train_dataset), len(validation_dataset), len(test_dataset))\n",
    "\n",
    "    print(\"#\" * 50, 3)\n",
    "\n",
    "    # 훈련 데이터 로더 생성 (배치 사이즈 32, 섞기 활성화, 마지막 배치 버리기)\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        drop_last=True  # 마지막 배치가 배치 크기보다 작으면 버림\n",
    "    )\n",
    "\n",
    "    # 데이터 로더의 각 배치 출력\n",
    "    for idx, batch in enumerate(train_data_loader):\n",
    "        input, target = batch\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34141e12-1d2a-470f-ae4f-d34f81ad01f5",
   "metadata": {},
   "source": [
    "# n_time_series_dataset_dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b676790-efdd-45cc-9f4c-2819dddfe3cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# BASE_PATH 설정: 데이터 파일을 찾기 위한 기준 경로\n",
    "BASE_PATH = str(Path(os.getcwd()).resolve().parent.parent)\n",
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "\n",
    "class BikesDataset(Dataset):\n",
    "    def __init__(self, train=True, test_days=1):\n",
    "        self.train = train  # 훈련 세트 여부\n",
    "        self.test_days = test_days  # 테스트에 사용할 일수\n",
    "\n",
    "        # 자전거 대여 데이터 파일 경로\n",
    "        bikes_path = os.path.join(BASE_PATH, \"_00_data\", \"e_time-series-bike-sharing-dataset\", \"hour-fixed.csv\")\n",
    "\n",
    "        # CSV 파일에서 데이터를 읽어옴\n",
    "        bikes_numpy = np.loadtxt(\n",
    "            fname=bikes_path, dtype=np.float32, delimiter=\",\", skiprows=1,\n",
    "            converters={\n",
    "                1: lambda x: float(x[8:10])  # 날짜에서 '07'을 가져와서 float으로 변환\n",
    "            }\n",
    "        )\n",
    "        bikes = torch.from_numpy(bikes_numpy)\n",
    "\n",
    "        # 데이터를 일별로 변환\n",
    "        daily_bikes = bikes.view(-1, 24, bikes.shape[1])  # daily_bikes.shape: [730, 24, 17]\n",
    "        self.daily_bikes_target = daily_bikes[:, :, -1].unsqueeze(dim=-1)  # 마지막 열을 타겟으로 설정\n",
    "\n",
    "        self.daily_bikes_data = daily_bikes[:, :, :-1]  # 나머지 데이터를 피처로 설정\n",
    "        eye_matrix = torch.eye(4)  # 날씨를 원-핫 인코딩하기 위한 단위 행렬\n",
    "\n",
    "        # 데이터 전처리: 일별 데이터에 날씨 원-핫 인코딩 추가\n",
    "        day_data_torch_list = []\n",
    "        for daily_idx in range(self.daily_bikes_data.shape[0]):  # range(730)\n",
    "            day = self.daily_bikes_data[daily_idx]  # 하루 데이터\n",
    "            weather_onehot = eye_matrix[day[:, 9].long() - 1]  # 날씨에 대한 원-핫 인코딩\n",
    "            day_data_torch = torch.cat(tensors=(day, weather_onehot), dim=1)  # 피처 결합\n",
    "            day_data_torch_list.append(day_data_torch)\n",
    "\n",
    "        self.daily_bikes_data = torch.stack(day_data_torch_list, dim=0)\n",
    "\n",
    "        # 9번째 열과 10번째 열 사이의 데이터를 결합\n",
    "        self.daily_bikes_data = torch.cat(\n",
    "            [self.daily_bikes_data[:, :, :9], self.daily_bikes_data[:, :, 10:]], dim=2\n",
    "        )\n",
    "\n",
    "        # 훈련 데이터와 타겟을 설정\n",
    "        total_length = len(self.daily_bikes_data)\n",
    "        self.train_bikes_data = self.daily_bikes_data[:total_length - test_days]\n",
    "        self.train_bikes_targets = self.daily_bikes_target[:total_length - test_days]\n",
    "\n",
    "        # 온도 정규화\n",
    "        train_temperatures = self.train_bikes_data[:, :, 9]\n",
    "        train_temperatures_mean = torch.mean(train_temperatures)\n",
    "        train_temperatures_std = torch.std(train_temperatures)\n",
    "        self.train_bikes_data[:, :, 9] = \\\n",
    "            (self.train_bikes_data[:, :, 9] - train_temperatures_mean) / train_temperatures_std\n",
    "\n",
    "        assert len(self.train_bikes_data) == len(self.train_bikes_targets)\n",
    "\n",
    "        # 테스트 데이터 설정\n",
    "        self.test_bikes_data = self.daily_bikes_data[-test_days:]\n",
    "        self.test_bikes_targets = self.daily_bikes_target[-test_days:]\n",
    "\n",
    "        # 테스트 데이터의 온도 정규화\n",
    "        self.test_bikes_data[:, :, 9] = \\\n",
    "            (self.test_bikes_data[:, :, 9] - train_temperatures_mean) / train_temperatures_std\n",
    "\n",
    "        assert len(self.test_bikes_data) == len(self.test_bikes_targets)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋 길이 반환\n",
    "        return len(self.train_bikes_data) if self.train else len(self.test_bikes_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 인덱스에 따라 데이터와 타겟 반환\n",
    "        bike_feature = self.train_bikes_data[idx] if self.train else self.test_bikes_data[idx]\n",
    "        bike_target = self.train_bikes_targets[idx] if self.train else self.test_bikes_targets[idx]\n",
    "        return bike_feature, bike_target\n",
    "\n",
    "    def __str__(self):\n",
    "        # 데이터셋의 정보 문자열 반환\n",
    "        if self.train:\n",
    "            str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "                len(self.train_bikes_data), self.train_bikes_data.shape, self.train_bikes_targets.shape\n",
    "            )\n",
    "        else:\n",
    "            str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "                len(self.test_bikes_data), self.test_bikes_data.shape, self.test_bikes_targets.shape\n",
    "            )\n",
    "        return str\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 훈련 데이터셋 인스턴스 생성\n",
    "    train_bikes_dataset = BikesDataset(train=True, test_days=1)\n",
    "    print(train_bikes_dataset)\n",
    "\n",
    "    print(\"#\" * 50, 1)\n",
    "\n",
    "    # 훈련 데이터셋과 검증 데이터셋으로 분할\n",
    "    train_dataset, validation_dataset = random_split(train_bikes_dataset, [0.8, 0.2])\n",
    "\n",
    "    print(\"[TRAIN]\")\n",
    "    for idx, sample in enumerate(train_dataset):\n",
    "        input, target = sample\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "    # 훈련 데이터 로더 생성\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "    for idx, batch in enumerate(train_data_loader):\n",
    "        input, target = batch\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "    print(\"#\" * 50, 2)\n",
    "\n",
    "    print(\"[VALIDATION]\")\n",
    "    for idx, sample in enumerate(validation_dataset):\n",
    "        input, target = sample\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "    # 검증 데이터 로더 생성\n",
    "    validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=32)\n",
    "\n",
    "    for idx, batch in enumerate(validation_data_loader):\n",
    "        input, target = batch\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "    print(\"#\" * 50, 3)\n",
    "\n",
    "    # 테스트 데이터셋 인스턴스 생성\n",
    "    test_dataset = BikesDataset(train=False, test_days=1)\n",
    "    print(test_dataset)\n",
    "\n",
    "    print(\"[TEST]\")\n",
    "    for idx, sample in enumerate(test_dataset):\n",
    "        input, target = sample\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n",
    "\n",
    "    # 테스트 데이터 로더 생성\n",
    "    test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "    for idx, batch in enumerate(test_data_loader):\n",
    "        input, target = batch\n",
    "        print(\"{0} - {1}: {2}\".format(idx, input.shape, target.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1257aa63-100d-4ef1-84b2-158f4b968fd2",
   "metadata": {},
   "source": [
    "# o_hourly_bikes_sharing_dataset_dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e990587-9e05-4b52-baa8-a3e6b2515bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "BASE_PATH = str(Path(os.getcwd()).resolve().parent.parent)\n",
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "\n",
    "class HourlyBikesDataset(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "    assert len(self.X) == len(self.y)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    X = self.X[idx]\n",
    "    y = self.y[idx]\n",
    "    return X, y\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.X), self.X.shape, self.y.shape\n",
    "    )\n",
    "    return str\n",
    "\n",
    "\n",
    "def get_hourly_bikes_data(sequence_size=24, validation_size=96, test_size=24, y_normalizer=100):\n",
    "  bikes_path = os.path.join(BASE_PATH, \"_00_data\", \"e_time-series-bike-sharing-dataset\", \"hour-fixed.csv\")\n",
    "\n",
    "  bikes_numpy = np.loadtxt(\n",
    "    fname=bikes_path, dtype=np.float32, delimiter=\",\", skiprows=1,\n",
    "    converters={\n",
    "      1: lambda x: float(x[8:10])  # 2011-01-07 --> 07 --> 7\n",
    "    }\n",
    "  )\n",
    "  bikes_data = torch.from_numpy(bikes_numpy).to(torch.float) # >>> torch.Size([17520, 17])\n",
    "  bikes_target = bikes_data[:, -1].unsqueeze(dim=-1)  # 'cnt'\n",
    "  bikes_data = bikes_data[:, :-1]  # >>> torch.Size([17520, 16])\n",
    "\n",
    "  eye_matrix = torch.eye(4)\n",
    "\n",
    "  data_torch_list = []\n",
    "  for idx in range(bikes_data.shape[0]):  # range(730)\n",
    "    hour_data = bikes_data[idx]  # day.shape: [24, 17]\n",
    "    weather_onehot = eye_matrix[hour_data[9].long() - 1]\n",
    "    concat_data_torch = torch.cat(tensors=(hour_data, weather_onehot), dim=-1)  # day_torch.shape: [24, 21]\n",
    "    data_torch_list.append(concat_data_torch)\n",
    "\n",
    "  bikes_data = torch.stack(data_torch_list, dim=0)\n",
    "  bikes_data = torch.cat([bikes_data[:, 1:9], bikes_data[:, 10:]], dim=-1)\n",
    "  print(bikes_data.shape, \"!!!\")  # >>> torch.Size([17520, 18])\n",
    "\n",
    "  data_size = len(bikes_data) - sequence_size\n",
    "  train_size = data_size - (validation_size + test_size)\n",
    "\n",
    "  #################################################################################################\n",
    "\n",
    "  row_cursor = 0\n",
    "\n",
    "  X_train_list = []\n",
    "  y_train_regression_list = []\n",
    "  for idx in range(0, train_size):\n",
    "    sequence_data = bikes_data[idx: idx + sequence_size]\n",
    "    sequence_target = bikes_target[idx + sequence_size - 1]\n",
    "    X_train_list.append(sequence_data)\n",
    "    y_train_regression_list.append(sequence_target)\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_train = torch.stack(X_train_list, dim=0).to(torch.float)\n",
    "  y_train_regression = torch.tensor(y_train_regression_list, dtype=torch.float32) / y_normalizer\n",
    "\n",
    "  m = X_train.mean(dim=0, keepdim=True)\n",
    "  s = X_train.std(dim=0, keepdim=True)\n",
    "  X_train = (X_train - m) / s\n",
    "\n",
    "  #################################################################################################\n",
    "\n",
    "  X_validation_list = []\n",
    "  y_validation_regression_list = []\n",
    "  for idx in range(row_cursor, row_cursor + validation_size):\n",
    "    sequence_data = bikes_data[idx: idx + sequence_size]\n",
    "    sequence_target = bikes_target[idx + sequence_size - 1]\n",
    "    X_validation_list.append(sequence_data)\n",
    "    y_validation_regression_list.append(sequence_target)\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_validation = torch.stack(X_validation_list, dim=0).to(torch.float)\n",
    "  y_validation_regression = torch.tensor(y_validation_regression_list, dtype=torch.float32) / y_normalizer\n",
    "\n",
    "  X_validation -= m\n",
    "  X_validation /= s\n",
    "  #################################################################################################\n",
    "\n",
    "  X_test_list = []\n",
    "  y_test_regression_list = []\n",
    "  for idx in range(row_cursor, row_cursor + test_size):\n",
    "    sequence_data = bikes_data[idx: idx + sequence_size]\n",
    "    sequence_target = bikes_target[idx + sequence_size - 1]\n",
    "    X_test_list.append(sequence_data)\n",
    "    y_test_regression_list.append(sequence_target)\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_test = torch.stack(X_test_list, dim=0).to(torch.float)\n",
    "  y_test_regression = torch.tensor(y_test_regression_list, dtype=torch.float32) / y_normalizer\n",
    "\n",
    "  X_test -= m\n",
    "  X_test /= s\n",
    "\n",
    "  return (\n",
    "    X_train, X_validation, X_test,\n",
    "    y_train_regression, y_validation_regression, y_test_regression\n",
    "  )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  X_train, X_validation, X_test, y_train, y_validation, y_test = get_hourly_bikes_data(\n",
    "    sequence_size=24, validation_size=96, test_size=24, y_normalizer=100\n",
    "  )\n",
    "\n",
    "  print(\"Train: {0}, Validation: {1}, Test: {2}\".format(len(X_train), len(X_validation), len(X_test)))\n",
    "\n",
    "  train_hourly_bikes_dataset = HourlyBikesDataset(X=X_train, y=y_train)\n",
    "  validation_hourly_bikes_dataset = HourlyBikesDataset(X=X_validation, y=y_validation)\n",
    "  test_houly_bikes_dataset = HourlyBikesDataset(X=X_test, y=y_test)\n",
    "\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=train_hourly_bikes_dataset, batch_size=32, shuffle=True, drop_last=True\n",
    "  )\n",
    "\n",
    "  # for idx, batch in enumerate(train_data_loader):\n",
    "  #   input, target = batch\n",
    "  #   print(\"{0} - {1}: {2}, {3}\".format(idx, input.shape, target.shape, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3066ddc9-6493-4f70-9d16-e7a3d36d463e",
   "metadata": {},
   "source": [
    "# p_cryptocurrency_dataset_dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef21434f-7d40-4901-ae70-6aa6303eb445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/cryptocurrency-price-prediction-using-deep-learning-70cfca50dd3a\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "BASE_PATH = str(Path(os.getcwd()).resolve().parent.parent)\n",
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "\n",
    "class CryptoCurrencyDataset(Dataset):\n",
    "  def __init__(self, X, y, is_regression=True):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "    assert len(self.X) == len(self.y)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    X = self.X[idx]\n",
    "    y = self.y[idx]\n",
    "    return X, y\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.X), self.X.shape, self.y.shape\n",
    "    )\n",
    "    return str\n",
    "\n",
    "\n",
    "def get_cryptocurrency_data(\n",
    "    sequence_size=10, validation_size=100, test_size=10, target_column='Close', y_normalizer=1.0e7, is_regression=True\n",
    "):\n",
    "  btc_krw_path = os.path.join(BASE_PATH, \"_00_data\", \"k_cryptocurrency\", \"BTC_KRW.csv\")\n",
    "  df = pd.read_csv(btc_krw_path)\n",
    "  row_size = len(df)\n",
    "  # ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "  date_list = df['Date']\n",
    "\n",
    "  df = df.drop(columns=['Date'])\n",
    "\n",
    "  data_size = row_size - sequence_size\n",
    "  train_size = data_size - (validation_size + test_size)\n",
    "  #################################################################################################\n",
    "\n",
    "  row_cursor = 0\n",
    "\n",
    "  X_train_list = []\n",
    "  y_train_regression_list = []\n",
    "  y_train_classification_list = []\n",
    "  y_train_date = []\n",
    "  for idx in range(0, train_size):\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values  # sequence_data.shape: (sequence_size, 5)\n",
    "    X_train_list.append(torch.from_numpy(sequence_data))\n",
    "    y_train_regression_list.append(df.iloc[idx + sequence_size][target_column])\n",
    "    y_train_classification_list.append(\n",
    "      1 if df.iloc[idx + sequence_size][target_column] >= df.iloc[idx + sequence_size - 1][target_column] else 0\n",
    "    )\n",
    "    y_train_date.append(date_list[idx + sequence_size])\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_train = torch.stack(X_train_list, dim=0).to(torch.float)\n",
    "  y_train_regression = torch.tensor(y_train_regression_list, dtype=torch.float32) / y_normalizer\n",
    "  y_train_classification = torch.tensor(y_train_classification_list, dtype=torch.int64)\n",
    "\n",
    "  m = X_train.mean(dim=0, keepdim=True)\n",
    "  s = X_train.std(dim=0, keepdim=True)\n",
    "  X_train = (X_train - m) / s\n",
    "\n",
    "  #################################################################################################\n",
    "\n",
    "  X_validation_list = []\n",
    "  y_validation_regression_list = []\n",
    "  y_validation_classification_list = []\n",
    "  y_validation_date = []\n",
    "  for idx in range(row_cursor, row_cursor + validation_size):\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values  # sequence_data.shape: (sequence_size, 5)\n",
    "    X_validation_list.append(torch.from_numpy(sequence_data))\n",
    "    y_validation_regression_list.append(df.iloc[idx + sequence_size][target_column])\n",
    "    y_validation_classification_list.append(\n",
    "      1 if df.iloc[idx + sequence_size][target_column] >= df.iloc[idx + sequence_size - 1][target_column] else 0\n",
    "    )\n",
    "    y_validation_date.append(date_list[idx + sequence_size])\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_validation = torch.stack(X_validation_list, dim=0).to(torch.float)\n",
    "  y_validation_regression = torch.tensor(y_validation_regression_list, dtype=torch.float32) / y_normalizer\n",
    "  y_validation_classification = torch.tensor(y_validation_classification_list, dtype=torch.int64)\n",
    "\n",
    "  X_validation = (X_validation - m) / s\n",
    "  #################################################################################################\n",
    "\n",
    "  X_test_list = []\n",
    "  y_test_regression_list = []\n",
    "  y_test_classification_list = []\n",
    "  y_test_date = []\n",
    "  for idx in range(row_cursor, row_cursor + test_size):\n",
    "    sequence_data = df.iloc[idx: idx + sequence_size].values  # sequence_data.shape: (sequence_size, 5)\n",
    "    X_test_list.append(torch.from_numpy(sequence_data))\n",
    "    y_test_regression_list.append(df.iloc[idx + sequence_size][target_column])\n",
    "    y_test_classification_list.append(\n",
    "      1 if df.iloc[idx + sequence_size][target_column] > df.iloc[idx + sequence_size - 1][target_column] else 0\n",
    "    )\n",
    "    y_test_date.append(date_list[idx + sequence_size])\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_test = torch.stack(X_test_list, dim=0).to(torch.float)\n",
    "  y_test_regression = torch.tensor(y_test_regression_list, dtype=torch.float32) / y_normalizer\n",
    "  y_test_classification = torch.tensor(y_test_classification_list, dtype=torch.int64)\n",
    "\n",
    "  X_test = (X_test - m) / s\n",
    "\n",
    "  if is_regression:\n",
    "    return (\n",
    "      X_train, X_validation, X_test,\n",
    "      y_train_regression, y_validation_regression, y_test_regression,\n",
    "      y_train_date, y_validation_date, y_test_date\n",
    "    )\n",
    "  else:\n",
    "    return (\n",
    "      X_train, X_validation, X_test,\n",
    "      y_train_classification, y_validation_classification, y_test_classification,\n",
    "      y_train_date, y_validation_date, y_test_date\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  is_regression = False\n",
    "\n",
    "  X_train, X_validation, X_test, y_train, y_validation, y_test, y_train_date, y_validation_date, y_test_date \\\n",
    "    = get_cryptocurrency_data(\n",
    "    sequence_size=10, validation_size=100, test_size=10,\n",
    "    target_column='Close', y_normalizer=1.0e7, is_regression=is_regression\n",
    "  )\n",
    "\n",
    "  train_crypto_currency_dataset = CryptoCurrencyDataset(X=X_train, y=y_train, is_regression=is_regression)\n",
    "  validation_crypto_currency_dataset = CryptoCurrencyDataset(X=X_validation, y=y_validation, is_regression=is_regression)\n",
    "  test_crypto_currency_dataset = CryptoCurrencyDataset(X=X_test, y=y_test, is_regression=is_regression)\n",
    "\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=train_crypto_currency_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    "  )\n",
    "\n",
    "  for idx, batch in enumerate(train_data_loader):\n",
    "    input, target = batch\n",
    "    print(\"{0} - {1}: {2}, {3}\".format(idx, input.shape, target.shape, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fd962d-96cf-4d1c-b21f-0c53efc5d5f0",
   "metadata": {},
   "source": [
    "# 숙제 후기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9638bc38-bd58-4b33-97c8-44868814de00",
   "metadata": {},
   "source": [
    "이번 \"_03_real_world_data_to_tensors\" 과제를 통해 실제 데이터를 텐서로 변환시키는 것을 경험했습니다. 다양한 데이터셋을 다루며, 데이터 전처리와 정규화의 중요성을 깊이 이해할 수 있었습니다. PyTorch의 Dataset과 DataLoader 클래스를 활용하여 데이터 로딩과 배치 처리를 효과적으로 수행해보았고. 실습을 통해 데이터의 형태와 구조를 변경하는 기술도 익히고, 이를 통해 머신러닝 모델의 학습 데이터 준비 과정의 핵심을 깨달았습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
